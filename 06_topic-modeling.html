
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Topic Modeling &#8212; Getting Started with Textual Data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="5. Clustering and Classification" href="05_clustering-and-classification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/datalab-logo-full-color-rgb.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Getting Started with Textual Data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Overview
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_logistics.html">
   1. Before We Begin…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_from-text-to-data.html">
   2. From Text to Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_cleaning-and-counting.html">
   3. Cleaning and Counting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_corpus-analytics.html">
   4. Corpus Analytics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_clustering-and-classification.html">
   5. Clustering and Classification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Topic Modeling
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/06_topic-modeling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ucdavisdatalab/workshop_getting_started_with_textual_data"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ucdavisdatalab/workshop_getting_started_with_textual_data/master?urlpath=tree/06_topic-modeling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topic-modeling-introduction">
   6.1. Topic Modeling: Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preliminiaries">
   6.2. Preliminiaries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-topic-model">
   6.3. Building a Topic Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-a-model">
     6.3.1. Initializing a model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-model">
     6.3.2. Training a model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspecting-the-results">
     6.3.3. Inspecting the results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fine-tuning-the-basics">
   6.4. Fine Tuning: The Basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting-the-number-of-topics">
     6.4.1. Setting the number of topics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perplexity">
     6.4.2. Perplexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coherence">
     6.4.3. Coherence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fine-tuning-advanced">
   6.5. Fine Tuning: Advanced
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters-alpha-and-eta">
     6.5.1. Hyperparameters: alpha and eta
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deciding-on-hyperparameter-values">
     6.5.2. Deciding on hyperparameter values
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploring-a-model">
   6.6. Exploring a Model
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="topic-modeling">
<h1><span class="section-number">6. </span>Topic Modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">¶</a></h1>
<p>In our sessions so far, we have used a number of different textual formalisms: tokens, n-grams, documents, vector
space models. Each has provided a unique vantage from which to begin exploring a corpus. Sometimes this vantage is
tightly locked to the material ‘thingness’ of texts: tokens and n-grams, for example, help us produce metrics about
the raw distribution of words. Others, however, were more abstract: a vector space model opens out to a view of
textual similarity, one that touches on concepts like genre, style, or a kind of nebulous ‘aboutness.’</p>
<p>This final chapter will work in a similar vein in its focus on topics. We know that texts are ‘about’ something.
They have a particular subject; a theme; a message; an argument, even. Often, texts are about several things; a
corpus, then, may contain dozens of contrasting themes, each of which we would want to identify – ideally at scale
– to understand the overall contours of that corpus. Using <strong>topic modeling</strong>, we can infer latent structures in a
corpus that often map onto these themes (albeit not always in a direct manner). These models produce groups, or
topics, of related words in a corpus. Taken together, the words in each topic suggest a theme, and we can thus use
topic modeling to probabalistically measure the association of a text with a particular topic.</p>
<p>As we’ll discuss below, human interpretation plays a key role in this process: topic models produce textual
structures, but it’s on us to give those structures meaning. Doing so is an interative process, in which we <strong>fine
tune</strong> various aspects of a model to effectively represent our corpus. This chapter will show you how to build a
model, how to appraise it, and how to start iterating through the process of fine tuning to produce a model that
best serves your research questions.</p>
<p>To do so, we’ll use a corpus of book blurbs sampled from the U. Hamburg Language Technology Group’s <a class="reference external" href="https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html">Blurb Genre
Collection</a>. The collection contains ~92,000 blurbs from Penguin Random House, ranging from Colson Whitehead’s
books to steamy supermarket romances and self-help manuals. We’ll use just 1,500 – not so much that we’d be stuck
waiting for hours for models to train, but enough to get a broad sense of different topics among the blurbs.</p>
<div class="admonition-learning-objectives admonition">
<p class="admonition-title">Learning Objectives</p>
<p>By the end of this chapter, you will be able to:</p>
<ul class="simple">
<li><p>Explain what a topic model is, what it represents, and how to use one to explore a corpus</p></li>
<li><p>Build a topic model</p></li>
<li><p>Use two scoring metrics, perplexity and coherence, to appraise the quality of a model</p></li>
<li><p>Understand how to improve a model by fine tuning its number of topics and its hyperparameter values</p></li>
</ul>
</div>
<div class="section" id="topic-modeling-introduction">
<h2><span class="section-number">6.1. </span>Topic Modeling: Introduction<a class="headerlink" href="#topic-modeling-introduction" title="Permalink to this headline">¶</a></h2>
<div class="margin sidebar">
<p class="sidebar-title">Citations</p>
<p>This description of topic modeling is influenced by Ted Underwood’s very effective <a class="reference external" href="https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/">explanation</a>. This <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">review article</a>, from David M. Blei (one of the original developers of the method), is also quite helpful.</p>
</div>
<p>There are a few different flavors of topic models. We’ll be using the most popular one, a <strong>latent Dirichlet
allocation</strong>, or LDA, model. It involves two assumptions: 1) documents are comprised of a mixture of topics; 2)
topics are comprised of a mixture of words. An LDA model represents these mixtures in terms of probability
distributions: a given passage, with a given set of words, is more or less likely to be about a particular topic,
which is in turn more or less likely to be made up of a certain grouping of words.</p>
<p>We initialize a model by predefining how many topics we think it should find. When the model begins training, it
randomly guesses which words are most associated with which topic. But over the course of its training, it will
start to keep track of the probabilities of recurrent word collocations: <em>river</em>, <em>bank</em>, and <em>water</em>, for example,
might keep showing up together. This suggests some coherence, a possible grouping of words. A second topic, on the
other hand, might have words like <em>money</em>, <em>bank</em>, and <em>robber</em>. The challenge here is that words belong to
multiple topics. In this instance, given a single instance of <em>bank</em>, it could be in either the first or second
topic. Given this, how does the model tell which topic a document containing the word <em>bank</em> is more strongly
associated with?</p>
<p>It does two things. First, the model tracks how often <em>bank</em> appears with its various collocates in the corpus. If
<em>bank</em> is generally more likely to appear with <em>river</em> and <em>water</em> than <em>money</em> and <em>robber</em>, this weights the
probability that this particular instance of <em>bank</em> belongs to the first topic. To put a check on this weighting,
the model also tracks how often <em>bank</em>’s collocates appear in the document in question. If, in this document,
<em>river</em> and <em>water</em> appear more often than <em>robber</em> and <em>money</em>, then that’s going to weight this instance of <em>bank</em> even further toward the first topic, not the second (it may even be that the latter two words don’t appear
in the document in all).</p>
<p>Using these weightings as a basis, the model will assign a probability score for a document’s association with
these two topics. This assignment will also inform the overall probability distribution of topics to words, which
will then inform further document-topic associations, and so on. Over the course of this process, topics will
become more consistent and focused and their associations with documents will become stronger and weaker, as
appropriate.</p>
<p>Here’s the formula that summarizes this process. Given topic <span class="math notranslate nohighlight">\(T\)</span>, word <span class="math notranslate nohighlight">\(W\)</span>, and document <span class="math notranslate nohighlight">\(D\)</span>, we determine the
probability of <span class="math notranslate nohighlight">\(W\)</span> belonging to <span class="math notranslate nohighlight">\(T\)</span> with:</p>
<div class="margin sidebar">
<p class="sidebar-title">Those two other letters…</p>
<p>These represent hyperparameters, which we’ll discuss below.</p>
</div>
<div class="math notranslate nohighlight">
\[
P(T|W,D) = \frac{\text{# of $W$ in } T + \eta_W}{\text{total tokens in } T + \eta} 
\cdot (\text{# words in $D$ that belong to } T + \alpha)
\]</div>
</div>
<div class="section" id="preliminiaries">
<h2><span class="section-number">6.2. </span>Preliminiaries<a class="headerlink" href="#preliminiaries" title="Permalink to this headline">¶</a></h2>
<p>Before we begin building a model ourselves, we’ll load in our data. As before, we’ll use a file manifest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">manifest</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/session_three/manifest.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;PUB_DATE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;PUB_DATE&#39;</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of blurbs: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">manifest</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Date range: </span><span class="si">{</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;PUB_DATE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2">--</span><span class="si">{</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;PUB_DATE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Genres: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;GENRE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>

<span class="n">manifest</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of blurbs: 1500 
Date range: 1958--2018 
Genres: Fiction, Classics, Nonfiction, Children’s Books, Teen &amp; Young Adult, Poetry, Humor
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AUTHOR</th>
      <th>TITLE</th>
      <th>GENRE</th>
      <th>PUB_DATE</th>
      <th>ISBN</th>
      <th>FILE_NAME</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>763</th>
      <td>Edd Doerr</td>
      <td>The Case Against School Vouchers</td>
      <td>Nonfiction</td>
      <td>1996-09-01</td>
      <td>9781573920926</td>
      <td>0763.txt</td>
    </tr>
    <tr>
      <th>1213</th>
      <td>Shawn Frederick</td>
      <td>Digital Photography</td>
      <td>Nonfiction</td>
      <td>2013-09-03</td>
      <td>9781615644131</td>
      <td>1213.txt</td>
    </tr>
    <tr>
      <th>900</th>
      <td>John Flannery</td>
      <td>Beard Boy</td>
      <td>Children’s Books</td>
      <td>2016-05-10</td>
      <td>9780698185463</td>
      <td>0900.txt</td>
    </tr>
    <tr>
      <th>178</th>
      <td>Christopher Moore</td>
      <td>Champlain</td>
      <td>Children’s Books</td>
      <td>2004-08-24</td>
      <td>9780887766572</td>
      <td>0178.txt</td>
    </tr>
    <tr>
      <th>765</th>
      <td>Rosemary Wells</td>
      <td>Bunny Cakes</td>
      <td>Children’s Books</td>
      <td>2000-02-01</td>
      <td>9780140566673</td>
      <td>0765.txt</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">manifest</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;PUB_DATE&#39;</span><span class="p">)[</span><span class="s1">&#39;ISBN&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                                                  <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Books per Year&quot;</span><span class="p">,</span>
                                                  <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Publication Year&quot;</span><span class="p">,</span>
                                                  <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Number of Books&quot;</span><span class="p">,</span>
                                                  <span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
                                                  <span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_topic-modeling_4_0.png" src="_images/06_topic-modeling_4_0.png" />
</div>
</div>
<p>To make file loading easier, we’ll isolate the file names from <code class="docutils literal notranslate"><span class="pre">manifest</span></code> and create a list of paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">indir</span> <span class="o">=</span> <span class="s2">&quot;data/session_three/input/&quot;</span>
<span class="n">paths</span> <span class="o">=</span> <span class="n">indir</span> <span class="o">+</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;FILE_NAME&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="building-a-topic-model">
<h2><span class="section-number">6.3. </span>Building a Topic Model<a class="headerlink" href="#building-a-topic-model" title="Permalink to this headline">¶</a></h2>
<p>With this bit of preliminary work done, we’re ready to build a topic model. There are numerous implementations of
LDA modeling available, ranging from the command line utility, <a class="reference external" href="https://mimno.github.io/Mallet/">MALLET</a> (which many of us in the DataLab use in our
own work), to built-in APIs offered by both <code class="docutils literal notranslate"><span class="pre">gensim</span></code> and <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. It’s tempting to use the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>
API: we should be familiar with this package’s conventions by now, and indeed it’s quite easy to spin up a topic
model using its API. But there’s a <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/issues/6777">reported bug</a> in a key metric for validating LDA models in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, and,
as far as we know, this bug hasn’t been fixed. Depending on your use case, this may not be a big deal. The bug has
to do with generating a <strong>perplexity score</strong> from the model, which is useful for fine tuning. You, however, may not
want to go through this process, especially if you’re working in an exploratory model. In this case, it’s probably
fine to use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<p>As for us: we’ll be demonstrating how to fine tune models and will thus avoid <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> for the workshop.
Instead, we’ll use <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code>, a Python wrapper built around Tomato, a topic modeling tool built in C++. Its API is
fairly intuitive and comes with lots of options, which we’ll leverage to build the best model possible for our
data.</p>
<div class="section" id="initializing-a-model">
<h3><span class="section-number">6.3.1. </span>Initializing a model<a class="headerlink" href="#initializing-a-model" title="Permalink to this headline">¶</a></h3>
<p>Initializing a topic model with <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> is simple: just assign <code class="docutils literal notranslate"><span class="pre">LDAModel()</span></code> to a variable and declare the number
of topics the model will generate. As we’ll discuss below, determining how many topics to use is a matter of some
debate and complexity, but for now, we’ll just pick a number and move ahead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">n_topics</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we need to add our blurbs to the model. We’ll do so by using a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in conjunction with all the paths we
created above. The only catch here is that we need to split each blurb into a list of tokens (right now they’re
stored as text blobs).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">p</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of documents: 1500
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-a-model">
<h3><span class="section-number">6.3.2. </span>Training a model<a class="headerlink" href="#training-a-model" title="Permalink to this headline">¶</a></h3>
<p>Our model is now ready to be trained. Under the hood, this happens in an iterative fashion, so we need to set the
total number of iterations we’d like to use to do the training. With that set, it’s simply a matter of calling
<code class="docutils literal notranslate"><span class="pre">model.train()</span></code>.</p>
<div class="margin sidebar">
<p class="sidebar-title">Number of iterations</p>
<p>Deciding on how many iterations to use can take some tweaking on real-world data. You can assume that the number
set here will allow a model to properly <a class="reference external" href="https://docs.paperspace.com/machine-learning/wiki/convergence">converge</a> for this data.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="inspecting-the-results">
<h3><span class="section-number">6.3.3. </span>Inspecting the results<a class="headerlink" href="#inspecting-the-results" title="Permalink to this headline">¶</a></h3>
<p>With our model trained on our corpus, we can access some high-level information about the corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of unique words: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total number of tokens: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">num_words</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of unique words: 19,609 
Total number of tokens: 129,027
</pre></div>
</div>
</div>
</div>
<p>For each topic, we can get the words most associated with that topic. The accompanying score is the probability of
that word appearing in a given topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic_id</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tup</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">%)&quot;</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">top_words</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">+ </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">top_words</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic 0: 
+ book (0.0262%), story (0.0134%), new (0.0109%), reader (0.0101%), read (0.0078%)
Topic 1: 
+ book (0.0110%), use (0.0067%), guide (0.0065%), include (0.0063%), work (0.0060%)
Topic 2: 
+ novel (0.0090%), new (0.0088%), war (0.0073%), man (0.0065%), time (0.0063%)
Topic 3: 
+ life (0.0253%), story (0.0084%), live (0.0082%), year (0.0079%), new (0.0077%)
Topic 4: 
+ just (0.0102%), make (0.0097%), friend (0.0086%), get (0.0076%), day (0.0074%)
</pre></div>
</div>
</div>
</div>
<p>This seems to make intuitive sense: we’re dealing here with several hundred book blurbs, so <em>book</em>, <em>reader</em>, and
<em>new</em> are all words we’d expect to see.</p>
<p>Since each topic has a probability score for every word, it’s also possible to look at the total word distribution
for a topic with <code class="docutils literal notranslate"><span class="pre">get_word_dist()</span></code>. This outputs an array of probabilities, which is indexed in the same order as
<code class="docutils literal notranslate"><span class="pre">model.used_vocabs()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_dist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_word_dist</span><span class="p">(</span><span class="n">topic_id</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">word_dist</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>just      0.010161
make      0.009699
friend    0.008590
get       0.007574
day       0.007390
like      0.006743
school    0.006420
new       0.006050
girl      0.005958
home      0.005727
dtype: float32
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">get_topic_dist()</span></code> performs a similar function, but for a document:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_title</span> <span class="o">=</span> <span class="n">manifest</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">topic_distribution</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">random_title</span><span class="p">]</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Topic distribution for &#39;</span><span class="si">{</span><span class="n">manifest</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">random_title</span><span class="p">,</span> <span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">topic_distribution</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;+ Topic #</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">%&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic distribution for &#39;Nurse&#39;:
+ Topic #0: 0.13%
+ Topic #1: 0.05%
+ Topic #2: 0.16%
+ Topic #3: 0.61%
+ Topic #4: 0.05%
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> also offers some shorthand to produce the top topics for a document. Here, we’ll sample from <code class="docutils literal notranslate"><span class="pre">manifest</span></code>,
get the associated index, pipe it into our model object, and return the top topic for a blurb.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sampled_titles</span> <span class="o">=</span> <span class="n">manifest</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top topics for:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">sampled_titles</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">get_topics</span><span class="p">(</span><span class="n">top_n</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">topic</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;+ </span><span class="si">{</span><span class="n">manifest</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: #</span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top topics for:
+ Max &amp; His Friends/Snowball &amp; the Flushed Pets (Secret Life of Pets): #0 (0.59%)
+ Undecorate: #4 (0.33%)
+ Metzger&#39;s Dog: #2 (0.34%)
+ Finding True Love in a Man-Eat-Man World: #1 (0.50%)
+ George Mueller: #3 (0.57%)
</pre></div>
</div>
</div>
</div>
<p>It’s possible to get even more granular. Every word in a document has its own associated topic, which will change
depending on the document. This is about as close to context-sensitive semantics as we can get with this method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">[</span><span class="n">random_title</span><span class="p">]</span>
<span class="n">word_topic</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">topics</span><span class="p">),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;WORD&#39;</span><span class="p">,</span> <span class="s1">&#39;TOPIC&#39;</span><span class="p">])</span>
<span class="n">word_topic</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>TOPIC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>national</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>bestseller</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>million</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>copy</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>sell</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>shock</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>inspirational</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>bestseller</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>nurse</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>story</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>To zoom out again: with list comprehension we can get a topic probability distribution for each document in our
corpus. In the literature, this is called the <strong>theta</strong>. More informally, we refer to it as the document-topic
matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">all_topic_distributions</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">docs</span><span class="p">]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_topic_distributions</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">/=</span> <span class="n">theta</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span>
                     <span class="n">index</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">],</span>
                     <span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;TOPIC_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_topics</span><span class="p">))</span>
                    <span class="p">)</span>
<span class="n">theta</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TOPIC_0</th>
      <th>TOPIC_1</th>
      <th>TOPIC_2</th>
      <th>TOPIC_3</th>
      <th>TOPIC_4</th>
    </tr>
    <tr>
      <th>TITLE</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>After Atlas</th>
      <td>0.070882</td>
      <td>0.037925</td>
      <td>0.551278</td>
      <td>0.270304</td>
      <td>0.069611</td>
    </tr>
    <tr>
      <th>Ragged Dick and Struggling Upward</th>
      <td>0.271145</td>
      <td>0.385517</td>
      <td>0.106728</td>
      <td>0.156587</td>
      <td>0.080024</td>
    </tr>
    <tr>
      <th>The Shape of Snakes</th>
      <td>0.025769</td>
      <td>0.037725</td>
      <td>0.598909</td>
      <td>0.227834</td>
      <td>0.109763</td>
    </tr>
    <tr>
      <th>The Setting Sun</th>
      <td>0.126659</td>
      <td>0.060929</td>
      <td>0.265295</td>
      <td>0.494987</td>
      <td>0.052131</td>
    </tr>
    <tr>
      <th>Stink and the Shark Sleepover</th>
      <td>0.373784</td>
      <td>0.030956</td>
      <td>0.149338</td>
      <td>0.093316</td>
      <td>0.352606</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>Greetings From Angelus</th>
      <td>0.319557</td>
      <td>0.419171</td>
      <td>0.125248</td>
      <td>0.125449</td>
      <td>0.010575</td>
    </tr>
    <tr>
      <th>Peppa Pig's Pop-up Princess Castle</th>
      <td>0.607036</td>
      <td>0.010852</td>
      <td>0.083282</td>
      <td>0.168597</td>
      <td>0.130234</td>
    </tr>
    <tr>
      <th>What Should the Left Propose?</th>
      <td>0.059229</td>
      <td>0.598486</td>
      <td>0.101163</td>
      <td>0.223962</td>
      <td>0.017160</td>
    </tr>
    <tr>
      <th>Peter and the Wolf</th>
      <td>0.634375</td>
      <td>0.120051</td>
      <td>0.083268</td>
      <td>0.126519</td>
      <td>0.035787</td>
    </tr>
    <tr>
      <th>Macedonia</th>
      <td>0.350776</td>
      <td>0.074940</td>
      <td>0.189547</td>
      <td>0.267247</td>
      <td>0.117490</td>
    </tr>
  </tbody>
</table>
<p>1500 rows × 5 columns</p>
</div></div></div>
</div>
<p>It’s also often helpful to know how “large” each topic is. There’s a caveat here, however, in that each word in the
model technically belongs to each topic, so it’s somewhat of a heuristic to say that a topic’s size is <span class="math notranslate nohighlight">\(n\)</span> words.
<code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> derives the output below by multiplying each column of the theta matrix by the document lengths in the
corpus. It then sums the results for each topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_sizes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_count_by_topics</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of words per topic:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;+ Topic #</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> words&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of words per topic:
+ Topic #0: 23,727 words
+ Topic #1: 23,950 words
+ Topic #2: 30,303 words
+ Topic #3: 29,590 words
+ Topic #4: 21,457 words
</pre></div>
</div>
</div>
</div>
<p>Finally, using the <code class="docutils literal notranslate"><span class="pre">num_words</span></code> attribute we can express this in terms of percentages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Topic proportion across the corpus:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;+ Topic #</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">num_words</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic proportion across the corpus:
+ Topic #0: 0.18%
+ Topic #1: 0.19%
+ Topic #2: 0.23%
+ Topic #3: 0.23%
+ Topic #4: 0.17%
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="fine-tuning-the-basics">
<h2><span class="section-number">6.4. </span>Fine Tuning: The Basics<a class="headerlink" href="#fine-tuning-the-basics" title="Permalink to this headline">¶</a></h2>
<p>All this looks good so far, but our topics are fairly general. More, their total proportions across the corpus are
relatively homogenous. Maybe this reflects what’s in our corpus – or maybe not. For example, a closer inspection of
individual word-topic distributions for a document show some mismatches between the topic, as we might interpret it
in a general sense, and those words.</p>
<p>If we get the word-topic distributions from above…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_topic</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>TOPIC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>national</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>bestseller</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>million</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>copy</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>sell</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>shock</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>inspirational</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>bestseller</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>nurse</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>story</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>…and add in the top few words for each topic assignment…</p>
<div class="margin sidebar">
<p class="sidebar-title">What this loop does</p>
<p>For each word in the blurb:</p>
<ol class="simple">
<li><p>Get its associated topic</p></li>
<li><p>Get the top five words for that topic</p></li>
<li><p>Join those words into a comma-separated string</p></li>
<li><p>Append that string to a column list, which we can add to our dataframe</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_words_column</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">word_topic</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">topic</span> <span class="o">=</span> <span class="n">word_topic</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;TOPIC&#39;</span><span class="p">]</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">top_words</span><span class="p">])</span>
    <span class="n">top_words_column</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">top_words</span><span class="p">)</span>
    
<span class="n">word_topic</span><span class="p">[</span><span class="s1">&#39;TOP_TOPIC_WORDS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_words_column</span>
<span class="n">word_topic</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WORD</th>
      <th>TOPIC</th>
      <th>TOP_TOPIC_WORDS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>national</td>
      <td>0</td>
      <td>book, story, new, reader, read</td>
    </tr>
    <tr>
      <th>1</th>
      <td>bestseller</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
    <tr>
      <th>2</th>
      <td>million</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
    <tr>
      <th>3</th>
      <td>copy</td>
      <td>0</td>
      <td>book, story, new, reader, read</td>
    </tr>
    <tr>
      <th>4</th>
      <td>sell</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
    <tr>
      <th>5</th>
      <td>shock</td>
      <td>2</td>
      <td>novel, new, war, man, time</td>
    </tr>
    <tr>
      <th>6</th>
      <td>inspirational</td>
      <td>1</td>
      <td>book, use, guide, include, work</td>
    </tr>
    <tr>
      <th>7</th>
      <td>bestseller</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
    <tr>
      <th>8</th>
      <td>nurse</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
    <tr>
      <th>9</th>
      <td>story</td>
      <td>3</td>
      <td>life, story, live, year, new</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>…it would appear that the actual words in the document do not really match with the top entries for its
associated topic. This suggests that we need to make some adjustments to the way we initialize our model so that it
better reflects the specificis of our corpus.</p>
<p>But there are several different parameters to adjust when intializing the model, so what, then, should we change?</p>
<div class="section" id="setting-the-number-of-topics">
<h3><span class="section-number">6.4.1. </span>Setting the number of topics<a class="headerlink" href="#setting-the-number-of-topics" title="Permalink to this headline">¶</a></h3>
<p>An easy answer would be the number of topics. If, as above, your topics seem too general, it may be because you’ve
too small a number of topics for the model. Increasing the number of topics you use may make the model more
interpretable.</p>
<p>We’ll show an example. But before doing so, we’ll load our files into a <code class="docutils literal notranslate"><span class="pre">Corpus()</span></code> object, which will streamline
the model initialization process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tomotopy.utils</span> <span class="kn">import</span> <span class="n">Corpus</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">Corpus</span><span class="p">()</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">p</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">corpus</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s set a higher number of topics for our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model_10</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">n_topics</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_10</span><span class="o">.</span><span class="n">docs</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of documents: 1500
</pre></div>
</div>
</div>
</div>
<p>And let’s also define a quick function to help us inspect the top words for each topic. This is the same <code class="docutils literal notranslate"><span class="pre">for</span></code> loop
we used earlier wrapped up in a callable function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_topic_words</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
        <span class="n">top_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic_id</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="n">top_n</span><span class="p">)</span>
        <span class="n">top_words</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tup</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.04f</span><span class="si">}</span><span class="s2">%)&quot;</span> <span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="n">top_words</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Topic #</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">+ </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">top_words</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With that done, we’ll train a model with our new number of topics and see what the results look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_10</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">print_topic_words</span><span class="p">(</span><span class="n">model_10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ new (0.0369%), time (0.0228%), author (0.0188%), york (0.0185%), novel (0.0172%)
Topic #1: 
+ book (0.0372%), story (0.0193%), reader (0.0151%), new (0.0134%), read (0.0108%)
Topic #2: 
+ book (0.0128%), use (0.0086%), offer (0.0083%), life (0.0080%), learn (0.0071%)
Topic #3: 
+ murder (0.0172%), mystery (0.0168%), crime (0.0090%), kill (0.0088%), killer (0.0088%)
Topic #4: 
+ food (0.0199%), recipe (0.0172%), italian (0.0080%), eat (0.0074%), cook (0.0074%)
Topic #5: 
+ life (0.0299%), woman (0.0165%), story (0.0154%), love (0.0151%), family (0.0126%)
Topic #6: 
+ history (0.0129%), work (0.0127%), american (0.0125%), world (0.0088%), year (0.0085%)
Topic #7: 
+ just (0.0120%), know (0.0100%), like (0.0089%), time (0.0088%), make (0.0085%)
Topic #8: 
+ war (0.0159%), world (0.0111%), power (0.0081%), battle (0.0081%), king (0.0069%)
Topic #9: 
+ child (0.0132%), little (0.0124%), animal (0.0109%), day (0.0107%), make (0.0094%)
</pre></div>
</div>
</div>
</div>
<p>That looks better! Adding more topics spreads out the word distributions.</p>
<p>Given that, what if we increased our number of topics even higher?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">model_30</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">n_topics</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>

<span class="n">model_30</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">print_topic_words</span><span class="p">(</span><span class="n">model_30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ life (0.0506%), book (0.0211%), live (0.0142%), experience (0.0133%), offer (0.0124%)
Topic #1: 
+ war (0.0329%), soldier (0.0166%), american (0.0133%), men (0.0133%), military (0.0112%)
Topic #2: 
+ baby (0.0162%), feel (0.0140%), fly (0.0129%), touch (0.0123%), rat (0.0112%)
Topic #3: 
+ friend (0.0323%), girl (0.0301%), love (0.0296%), old (0.0195%), brother (0.0150%)
Topic #4: 
+ new (0.0424%), book (0.0368%), time (0.0296%), york (0.0271%), review (0.0179%)
Topic #5: 
+ just (0.0196%), time (0.0159%), make (0.0146%), like (0.0145%), know (0.0132%)
Topic #6: 
+ guide (0.0242%), travel (0.0233%), top (0.0170%), new (0.0166%), eyewitness (0.0161%)
Topic #7: 
+ world (0.0338%), human (0.0310%), new (0.0224%), people (0.0152%), way (0.0142%)
Topic #8: 
+ star (0.0243%), war (0.0211%), lego (0.0170%), universe (0.0138%), set (0.0124%)
Topic #9: 
+ london (0.0222%), ben (0.0114%), lady (0.0107%), freud (0.0100%), lacey (0.0093%)
Topic #10: 
+ classic (0.0288%), work (0.0252%), english (0.0194%), penguin (0.0191%), literature (0.0180%)
Topic #11: 
+ god (0.0330%), magic (0.0210%), king (0.0191%), return (0.0158%), kingdom (0.0153%)
Topic #12: 
+ guide (0.0227%), use (0.0226%), book (0.0213%), step (0.0180%), learn (0.0163%)
Topic #13: 
+ people (0.0145%), show (0.0136%), business (0.0124%), think (0.0123%), study (0.0121%)
Topic #14: 
+ music (0.0222%), sport (0.0163%), team (0.0155%), star (0.0134%), baseball (0.0130%)
Topic #15: 
+ home (0.0270%), year (0.0245%), family (0.0219%), life (0.0193%), old (0.0177%)
Topic #16: 
+ book (0.0458%), reader (0.0166%), read (0.0156%), child (0.0155%), feature (0.0147%)
Topic #17: 
+ health (0.0171%), program (0.0128%), body (0.0124%), plan (0.0117%), help (0.0114%)
Topic #18: 
+ murder (0.0317%), town (0.0270%), mystery (0.0258%), death (0.0188%), police (0.0137%)
Topic #19: 
+ comic (0.0279%), cat (0.0228%), poem (0.0217%), animal (0.0199%), cole (0.0165%)
Topic #20: 
+ tale (0.0337%), horse (0.0263%), ship (0.0189%), adventure (0.0164%), sea (0.0144%)
Topic #21: 
+ christmas (0.0234%), tree (0.0168%), house (0.0156%), jack (0.0135%), night (0.0127%)
Topic #22: 
+ political (0.0253%), state (0.0200%), american (0.0149%), america (0.0144%), power (0.0121%)
Topic #23: 
+ story (0.0397%), year (0.0211%), world (0.0173%), life (0.0138%), become (0.0121%)
Topic #24: 
+ art (0.0255%), work (0.0189%), history (0.0158%), artist (0.0143%), life (0.0141%)
Topic #25: 
+ century (0.0257%), history (0.0161%), john (0.0138%), war (0.0130%), figure (0.0092%)
Topic #26: 
+ woman (0.0427%), love (0.0375%), life (0.0276%), mother (0.0167%), heart (0.0153%)
Topic #27: 
+ new (0.0268%), author (0.0216%), novel (0.0171%), time (0.0158%), series (0.0158%)
Topic #28: 
+ world (0.0165%), secret (0.0137%), know (0.0133%), face (0.0098%), dark (0.0096%)
Topic #29: 
+ recipe (0.0304%), food (0.0279%), family (0.0143%), cook (0.0133%), drink (0.0129%)
</pre></div>
</div>
</div>
</div>
<p>This also looks pretty solid. Between the two models, there appear to be some similar topics, but the second model,
which has a higher number of topics, includes a wider range of words in the top word distrubition. While all that
seems well and good, we don’t yet have a way to determine whether an increase in the number of topics will always
produce more interpretable results. At some point, we might start splitting hairs. In fact, we can see this
beginning to happen in a few instances with the second model. There are a few topics above that we might prefer to
merge into a single one. Maybe topics 4 and 27, for example, would be better off belonging together, rather than
staying apart, as they are here.</p>
<p>So the question is, what is an ideal number of topics?</p>
<p>One way to approach this question would be to run through a range of different topic sizes and inspect the results.
In some cases, it can be perfectly valid to pick the number of topics that appears to be the most interpretable for
you and the questions you have about your corpus. But there are also a few metrics we can use to measure the
quality of a given model in terms of the underlying data it represents. Sometimes these metrics lead to models that
aren’t quite as interpretable, but they also help us make a more empirically grounded assessment of the resultant
topics.</p>
</div>
<div class="section" id="perplexity">
<h3><span class="section-number">6.4.2. </span>Perplexity<a class="headerlink" href="#perplexity" title="Permalink to this headline">¶</a></h3>
<p>The first of these measures is <strong>perplexity</strong>. In text mining and natural language processing, we use perplexity
scoring to evaluate how well a model predicts an unseen set of words. Essentially, it measures how “surprised” a
model is by a sequence of unseen words. The lower the perplexity, the more your model is capable of mapping
predictions against the data it’s been trained on.</p>
<div class="margin sidebar">
<p class="sidebar-title">More on perplexity</p>
<p>If you’d like to read more on perplexity in the context of NLP, <a class="reference external" href="https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3">this post</a> offers a good walkthrough of the
concept and its application for the kind of work we’re doing here.</p>
</div>
<p>When you train a <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> model object, the model records a perplexity score for the training run. We can access
this score as an attribute for a given model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Perplexity score for the </span><span class="si">{</span><span class="n">model_10</span><span class="o">.</span><span class="n">k</span><span class="si">}</span><span class="s2">-topic model: </span><span class="si">{</span><span class="n">model_10</span><span class="o">.</span><span class="n">perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Perplexity score for the </span><span class="si">{</span><span class="n">model_30</span><span class="o">.</span><span class="n">k</span><span class="si">}</span><span class="s2">-topic model: </span><span class="si">{</span><span class="n">model_30</span><span class="o">.</span><span class="n">perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Perplexity score for the 10-topic model: 9767.1118 
Perplexity score for the 30-topic model: 10225.3969
</pre></div>
</div>
</div>
</div>
<p>Interestingly, in this instance, the model with the smaller number of topics has a better perplexity score than the
one with more topics. This would suggest that the first model is better fitted to our data and is thus a “better”
model.</p>
<p>It also suggests that there may be a topic number between these two that has an even better perplexity score. We
can test to see whether this is the case by constructing a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, in which we iterate through a range of
different topic numbers, train a model on each, and record the resultant perplexity scores.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_topic_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>

<span class="n">perplexity_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">n_topic_range</span><span class="p">:</span>
    <span class="n">_model</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
    <span class="n">_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">perplexity_scores</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;N_TOPICS&#39;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span> <span class="s1">&#39;PERPLEXITY_SCORE&#39;</span><span class="p">:</span> <span class="n">_model</span><span class="o">.</span><span class="n">perplexity</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s convert the results to a dataframe and, while we’re at it, train a model with the best-scoring number of
topics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perplexity_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">perplexity_scores</span><span class="p">)</span>
<span class="n">perplexity_scores</span> <span class="o">=</span> <span class="n">perplexity_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;PERPLEXITY_SCORE&#39;</span><span class="p">)</span>

<span class="n">best_n_topic</span> <span class="o">=</span> <span class="n">perplexity_scores</span><span class="o">.</span><span class="n">nsmallest</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;PERPLEXITY_SCORE&#39;</span><span class="p">)[</span><span class="s1">&#39;N_TOPICS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">best_p</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">best_n_topic</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
<span class="n">best_p</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perplexity_scores</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>N_TOPICS</th>
      <th>PERPLEXITY_SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>22</td>
      <td>9743.965507</td>
    </tr>
    <tr>
      <th>0</th>
      <td>10</td>
      <td>9767.111839</td>
    </tr>
    <tr>
      <th>17</th>
      <td>27</td>
      <td>9817.241300</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11</td>
      <td>9854.947514</td>
    </tr>
    <tr>
      <th>7</th>
      <td>17</td>
      <td>9901.255709</td>
    </tr>
    <tr>
      <th>18</th>
      <td>28</td>
      <td>9960.354987</td>
    </tr>
    <tr>
      <th>5</th>
      <td>15</td>
      <td>9980.877440</td>
    </tr>
    <tr>
      <th>13</th>
      <td>23</td>
      <td>9992.182970</td>
    </tr>
    <tr>
      <th>8</th>
      <td>18</td>
      <td>10016.706893</td>
    </tr>
    <tr>
      <th>14</th>
      <td>24</td>
      <td>10072.478196</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21</td>
      <td>10088.561813</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13</td>
      <td>10096.603287</td>
    </tr>
    <tr>
      <th>10</th>
      <td>20</td>
      <td>10097.970560</td>
    </tr>
    <tr>
      <th>6</th>
      <td>16</td>
      <td>10103.504066</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12</td>
      <td>10141.311590</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In this instance, it looks like 22 topics is the best, though the difference between the perplexity scores for that
model and the next best-scoring model, our 10-topic model, is relatively small. Given this, if you find that a
10-topic model is more interpretable, you may choose to make a compromise on perplexity and go with that instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_topic_words</span><span class="p">(</span><span class="n">best_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ book (0.0318%), story (0.0279%), time (0.0214%), new (0.0184%), novel (0.0166%)
Topic #1: 
+ love (0.0391%), woman (0.0364%), life (0.0232%), mother (0.0203%), family (0.0184%)
Topic #2: 
+ life (0.0400%), world (0.0225%), great (0.0157%), year (0.0137%), story (0.0134%)
Topic #3: 
+ just (0.0201%), like (0.0149%), make (0.0126%), want (0.0124%), know (0.0122%)
Topic #4: 
+ book (0.0269%), child (0.0241%), school (0.0190%), young (0.0186%), kid (0.0169%)
Topic #5: 
+ war (0.0349%), world (0.0182%), battle (0.0152%), fight (0.0131%), force (0.0105%)
Topic #6: 
+ new (0.0383%), york (0.0232%), art (0.0163%), music (0.0135%), street (0.0100%)
Topic #7: 
+ god (0.0361%), spiritual (0.0212%), book (0.0164%), religious (0.0112%), religion (0.0100%)
Topic #8: 
+ home (0.0218%), house (0.0215%), cat (0.0159%), horse (0.0148%), dog (0.0120%)
Topic #9: 
+ adventure (0.0244%), story (0.0237%), tale (0.0155%), book (0.0153%), feature (0.0129%)
Topic #10: 
+ work (0.0272%), classic (0.0211%), year (0.0168%), penguin (0.0154%), literature (0.0133%)
Topic #11: 
+ book (0.0268%), include (0.0163%), help (0.0130%), new (0.0126%), provide (0.0113%)
Topic #12: 
+ food (0.0269%), recipe (0.0257%), family (0.0111%), cook (0.0105%), italian (0.0102%)
Topic #13: 
+ city (0.0249%), london (0.0151%), empire (0.0107%), elizabeth (0.0099%), king (0.0096%)
Topic #14: 
+ new (0.0304%), author (0.0245%), time (0.0231%), bestselling (0.0221%), york (0.0197%)
Topic #15: 
+ travel (0.0194%), sea (0.0168%), new (0.0130%), top (0.0121%), map (0.0119%)
Topic #16: 
+ use (0.0326%), step (0.0179%), simple (0.0150%), book (0.0138%), create (0.0131%)
Topic #17: 
+ team (0.0175%), baseball (0.0170%), sport (0.0164%), hockey (0.0117%), player (0.0111%)
Topic #18: 
+ family (0.0138%), new (0.0136%), secret (0.0135%), know (0.0124%), old (0.0124%)
Topic #19: 
+ magic (0.0159%), king (0.0141%), return (0.0138%), kingdom (0.0116%), power (0.0105%)
Topic #20: 
+ american (0.0246%), history (0.0203%), political (0.0167%), america (0.0158%), state (0.0132%)
Topic #21: 
+ business (0.0122%), problem (0.0105%), success (0.0101%), health (0.0099%), people (0.0097%)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coherence">
<h3><span class="section-number">6.4.3. </span>Coherence<a class="headerlink" href="#coherence" title="Permalink to this headline">¶</a></h3>
<p>If you’re having trouble mapping perplexity scores onto interpretable results, you might use a <strong>coherence score</strong>
instead. Coherence scores measure the degree of semantic similarity among the words in a topic. Some people prefer
to use coherence scoring in place of perplexity because these scores help distinguish the difference between topics
that fit snugly on consistent word co-occurence and those that are artifacts of statistical inference.</p>
<p>There are a few ways to calculate coherence scores. We’ll use <code class="docutils literal notranslate"><span class="pre">c_v</span></code> coherence, which uses the two kinds of text
similarity we’ve already seen in the workshop: pointwise mutual information (PMI) and cosine similarity. This
method takes the co-occurence counts of top words in a given topic and calculates a PMI score for each word. Then,
it looks to every other topic in the model and calculates a PMI score for the present topic’s words and those in
the other topics. This results in a series of vectors, which are then measured with cosine similarity.</p>
<p>That’s a mouthful, but <code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> makes implementing it a breeze. Let’s look at the score for the best model above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tomotopy.coherence</span> <span class="kn">import</span> <span class="n">Coherence</span>

<span class="n">coherence</span> <span class="o">=</span> <span class="n">Coherence</span><span class="p">(</span><span class="n">best_p</span><span class="p">,</span> <span class="n">coherence</span> <span class="o">=</span> <span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coherence score: </span><span class="si">{</span><span class="n">coherence</span><span class="o">.</span><span class="n">get_score</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coherence score: 0.6051
</pre></div>
</div>
</div>
</div>
<p>Like with perplexity, we can construct a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop and look for the best score among a set of different topic
sizes. Here, we’re looking for the highest score, which will be a number between 0 and 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_topic_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>

<span class="n">coherence_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">n_topic_range</span><span class="p">:</span>
    <span class="n">_model</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
    <span class="n">_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">coherence</span> <span class="o">=</span> <span class="n">Coherence</span><span class="p">(</span><span class="n">_model</span><span class="p">,</span> <span class="n">coherence</span> <span class="o">=</span> <span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
    <span class="n">coherence_scores</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;N_TOPICS&#39;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span> <span class="s1">&#39;COHERENCE_SCORE&#39;</span><span class="p">:</span> <span class="n">coherence</span><span class="o">.</span><span class="n">get_score</span><span class="p">()})</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s format the scores, find the best one, and train a model on that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coherence_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coherence_scores</span><span class="p">)</span>
<span class="n">coherence_scores</span> <span class="o">=</span> <span class="n">coherence_scores</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;COHERENCE_SCORE&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">best_n_topic</span> <span class="o">=</span> <span class="n">coherence_scores</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;COHERENCE_SCORE&#39;</span><span class="p">)[</span><span class="s1">&#39;N_TOPICS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">best_c</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">best_n_topic</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
<span class="n">best_c</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coherence_scores</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>N_TOPICS</th>
      <th>COHERENCE_SCORE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>19</th>
      <td>29</td>
      <td>0.655928</td>
    </tr>
    <tr>
      <th>18</th>
      <td>28</td>
      <td>0.654988</td>
    </tr>
    <tr>
      <th>17</th>
      <td>27</td>
      <td>0.648859</td>
    </tr>
    <tr>
      <th>20</th>
      <td>30</td>
      <td>0.645488</td>
    </tr>
    <tr>
      <th>13</th>
      <td>23</td>
      <td>0.624086</td>
    </tr>
    <tr>
      <th>15</th>
      <td>25</td>
      <td>0.615611</td>
    </tr>
    <tr>
      <th>16</th>
      <td>26</td>
      <td>0.608920</td>
    </tr>
    <tr>
      <th>12</th>
      <td>22</td>
      <td>0.605131</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21</td>
      <td>0.599103</td>
    </tr>
    <tr>
      <th>14</th>
      <td>24</td>
      <td>0.595933</td>
    </tr>
    <tr>
      <th>8</th>
      <td>18</td>
      <td>0.579910</td>
    </tr>
    <tr>
      <th>10</th>
      <td>20</td>
      <td>0.578593</td>
    </tr>
    <tr>
      <th>5</th>
      <td>15</td>
      <td>0.570283</td>
    </tr>
    <tr>
      <th>7</th>
      <td>17</td>
      <td>0.558606</td>
    </tr>
    <tr>
      <th>9</th>
      <td>19</td>
      <td>0.558422</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Looks like a 29-topic model wins out! Here are the top words for each topic:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_topic_words</span><span class="p">(</span><span class="n">best_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ city (0.0212%), london (0.0172%), old (0.0162%), brother (0.0158%), miss (0.0142%)
Topic #1: 
+ win (0.0262%), award (0.0259%), story (0.0239%), novel (0.0202%), short (0.0165%)
Topic #2: 
+ family (0.0300%), life (0.0283%), mother (0.0237%), father (0.0236%), love (0.0219%)
Topic #3: 
+ world (0.0245%), story (0.0219%), life (0.0166%), year (0.0150%), great (0.0134%)
Topic #4: 
+ work (0.0291%), classic (0.0234%), literature (0.0190%), penguin (0.0180%), introduction (0.0176%)
Topic #5: 
+ new (0.0583%), time (0.0292%), school (0.0236%), help (0.0194%), series (0.0178%)
Topic #6: 
+ magic (0.0227%), poem (0.0191%), dragon (0.0151%), power (0.0131%), sword (0.0131%)
Topic #7: 
+ use (0.0158%), learn (0.0150%), help (0.0138%), think (0.0117%), business (0.0108%)
Topic #8: 
+ town (0.0238%), mystery (0.0235%), murder (0.0156%), small (0.0136%), detective (0.0113%)
Topic #9: 
+ music (0.0322%), song (0.0185%), band (0.0127%), jones (0.0111%), buddhist (0.0105%)
Topic #10: 
+ cole (0.0208%), van (0.0115%), say (0.0108%), freud (0.0100%), lacey (0.0093%)
Topic #11: 
+ drink (0.0183%), city (0.0178%), camp (0.0126%), hockey (0.0126%), country (0.0126%)
Topic #12: 
+ god (0.0520%), word (0.0165%), religious (0.0156%), japanese (0.0148%), jesus (0.0131%)
Topic #13: 
+ guide (0.0318%), use (0.0227%), new (0.0186%), include (0.0138%), step (0.0138%)
Topic #14: 
+ child (0.0267%), read (0.0239%), animal (0.0222%), reader (0.0207%), book (0.0177%)
Topic #15: 
+ history (0.0258%), century (0.0200%), american (0.0194%), political (0.0158%), state (0.0117%)
Topic #16: 
+ health (0.0196%), body (0.0136%), weight (0.0128%), plan (0.0113%), eat (0.0113%)
Topic #17: 
+ like (0.0181%), just (0.0179%), make (0.0149%), know (0.0138%), want (0.0137%)
Topic #18: 
+ book (0.0437%), adventure (0.0191%), series (0.0187%), feature (0.0177%), little (0.0153%)
Topic #19: 
+ power (0.0149%), country (0.0130%), america (0.0122%), financial (0.0109%), american (0.0106%)
Topic #20: 
+ war (0.0384%), battle (0.0157%), world (0.0144%), force (0.0138%), fight (0.0127%)
Topic #21: 
+ recipe (0.0334%), food (0.0281%), family (0.0148%), italian (0.0144%), cook (0.0144%)
Topic #22: 
+ human (0.0325%), science (0.0198%), planet (0.0178%), time (0.0170%), universe (0.0147%)
Topic #23: 
+ life (0.0303%), book (0.0242%), world (0.0147%), experience (0.0116%), practice (0.0101%)
Topic #24: 
+ time (0.0401%), book (0.0370%), new (0.0347%), york (0.0255%), write (0.0181%)
Topic #25: 
+ sea (0.0290%), ship (0.0259%), king (0.0208%), captain (0.0112%), high (0.0097%)
Topic #26: 
+ woman (0.0415%), love (0.0309%), life (0.0240%), heart (0.0197%), novel (0.0145%)
Topic #27: 
+ art (0.0404%), artist (0.0272%), work (0.0182%), include (0.0123%), writer (0.0109%)
Topic #28: 
+ dark (0.0137%), kill (0.0132%), dead (0.0128%), murder (0.0118%), killer (0.0106%)
</pre></div>
</div>
</div>
</div>
<p>And here’s a distribution plot of the topic proportions. We’ll wrap this code in a function for later comparison
work.</p>
<div class="margin sidebar">
<p class="sidebar-title">What this function does</p>
<p>For each topic in the model:</p>
<ol class="simple">
<li><p>Get the top five words</p></li>
<li><p>Use string formatting in concert with <code class="docutils literal notranslate"><span class="pre">join()</span></code> to label and stringify those words</p></li>
<li><p>Append the formatted string to an empty list, which we can use as an index in a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> series</p></li>
<li><p>Plot the resultant series</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_topic_proportions</span><span class="p">(</span><span class="n">tm</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">topic_proportions</span> <span class="o">=</span> <span class="n">tm</span><span class="o">.</span><span class="n">get_count_by_topics</span><span class="p">()</span> <span class="o">/</span> <span class="n">tm</span><span class="o">.</span><span class="n">num_words</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tm</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">tm</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="n">top_n</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Topic #</span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">: &quot;</span> <span class="o">+</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
        <span class="n">top_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        
    <span class="n">to_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">topic_proportions</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">top_words</span><span class="p">)</span>
    <span class="n">to_plot</span> <span class="o">=</span> <span class="n">to_plot</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
    <span class="n">to_plot</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span>
                      <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Topic Proportions for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                      <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Topic&quot;</span>
                     <span class="p">);</span>
    
<span class="n">plot_topic_proportions</span><span class="p">(</span><span class="n">best_c</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;Best Coherence Model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_topic-modeling_68_0.png" src="_images/06_topic-modeling_68_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="fine-tuning-advanced">
<h2><span class="section-number">6.5. </span>Fine Tuning: Advanced<a class="headerlink" href="#fine-tuning-advanced" title="Permalink to this headline">¶</a></h2>
<div class="section" id="hyperparameters-alpha-and-eta">
<h3><span class="section-number">6.5.1. </span>Hyperparameters: alpha and eta<a class="headerlink" href="#hyperparameters-alpha-and-eta" title="Permalink to this headline">¶</a></h3>
<p>Clearly, there is much to consider with respect to the number of topics alone. But this number is not the only
value we can set when initializing a model. LDA modeling has two key <strong>hyperparameters</strong>, which we can configure to
more specifically control the nature of the topics a training run produces:</p>
<div class="margin sidebar">
<p class="sidebar-title">Want more details?</p>
<p>This Stack Exchange <a class="reference external" href="https://datascience.stackexchange.com/a/202">answer</a> is a remarkably succinct summary of these two hyperparameters.</p>
</div>
<ul class="simple">
<li><p><strong>Alpha</strong>: represents document-topic density; the higher the alpha, the more evenly distributed, or “symmetric,”
topic proportions are in a particular document; a lower alpha means topic proportions are more “asymmetric” (that
is, a document will have fewer predominating topics, rather than several)</p></li>
<li><p><strong>Eta</strong>: represents word-topic density; the higher the eta, the more word probabilities will be distributed
evenly across a topic (specifically, this boosts the presence of low-probability words); a lower eta means word
distributions are more uneven, so each topic will have less dominant words</p></li>
</ul>
<p>Essentially, these two hyperparameters variously control specificity in our model: one for the way our model
handles document specificity and one for the way it handles topic specificity.</p>
<div class="admonition-on-terminology admonition">
<p class="admonition-title">On terminology</p>
<p>Different LDA implementations have different names for these hyperparameters. Eta, for example, is also referred to
as “beta.” When reading the documentation for an implementation, look for whatever term stands for the “document
prior” (alpha) and the “word prior” (eta).</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tomotopy</span></code> has actually been setting values for alpha and eta all along. If we initialize a new model, we can
inspect their default values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Alpha value: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Eta value: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">eta</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Alpha value: 0.10 
Eta value: 0.01
</pre></div>
</div>
</div>
</div>
<p>And we can declare specific values for each using arguments in <code class="docutils literal notranslate"><span class="pre">LDAModel()</span></code>. Let’s show an example. Below, we boost
the alpha and lessen the eta. With a high alpha, we expect to create a more even distribution in topics among the
documents, which will in turn mean each topic has more words associated with it; with a low eta, the probabilities
for the top words within these topics should be higher.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ae_adjusted</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
<span class="n">ae_adjusted</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s compare with the best coherence score model from above.</p>
<div class="margin sidebar">
<p class="sidebar-title">What this loop does</p>
<p>For each model in our model dictionary:</p>
<ol class="simple">
<li><p>Iterate through all the topics</p></li>
<li><p>Get the top five words for each topic</p></li>
<li><p>Use list comprehension to get the associated probability scores for those words</p></li>
<li><p>Print the median word count for the topics</p></li>
<li><p>Print the mean probability scores for the top words in the topic</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;best coherence&#39;</span><span class="p">:</span> <span class="n">best_c</span><span class="p">,</span> <span class="s1">&#39;high alpha/low eta&#39;</span><span class="p">:</span> <span class="n">ae_adjusted</span><span class="p">}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">tm</span> <span class="ow">in</span> <span class="n">model_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">probability_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tm</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">tm</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
        <span class="n">probability_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;For the </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> model:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">+ Median number of words per topic:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">tm</span><span class="o">.</span><span class="n">get_count_by_topics</span><span class="p">())</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">+ Mean probability for the top five words per topic:&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">probability_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span>
        <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>For the best coherence model: 
+ Median number of words per topic: 3568 
+ Mean probability for the top five words per topic: 0.0194% 

For the high alpha/low eta model: 
+ Median number of words per topic: 4559 
+ Mean probability for the top five words per topic: 0.0286% 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="deciding-on-hyperparameter-values">
<h3><span class="section-number">6.5.2. </span>Deciding on hyperparameter values<a class="headerlink" href="#deciding-on-hyperparameter-values" title="Permalink to this headline">¶</a></h3>
<p>As with setting our number of topics, we could spend hours going back and forth between different alpha and eta
values to determine which combination of hyperparameters is best. In the literature about LDA modeling, researchers
have suggested various ways of setting these values programmatically. For example, the authors of <a class="reference external" href="https://www.pnas.org/content/101/suppl_1/5228">this paper</a>
suggest that the ideal alpha and eta values are <span class="math notranslate nohighlight">\(\frac{50}{k}\)</span> and 0.1, respectively (where <span class="math notranslate nohighlight">\(k\)</span> is the number of
topics). Alternatively, in blogs and forums like Stack Exchange, you’ll often see people advocate for an approach
called <strong>grid searching</strong>. This involves selecting a range of different values for the hyperparameters, permuting
them, and building as many different models as it takes to go through all possible permutations.</p>
<p>Both of these approaches are reasonably valid, but they don’t emphasize an important point about what our
hyperparameters represent. Remember from above that both of them refer to <em>priors</em>, that is, to certain kinds of
knowledge we have about our data before we even model it. In our case, we’re working with a bunch of book blurbs.
The generic conventions of blurbs are fairly constrained, so it probably doesn’t make sense to raise our alpha
values – in a sense we’re already working with a “high alpha” corpus. The same might be said of a corpus of tweets
collected around a highly specific set of keywords: in this instance, your data collection is doing the work of
hyperparameter optimization. Put another way, <em>setting hyperparameters depends on your data and your research
question(s)</em>. It’s as valid to ask, “is this an interpretable model?”, or, “does this match with what I know about
my data?”, as it is to employ perplexity scores to define a good alpha or eta.</p>
<p>Here’s one example of what happens if you only fine tune with mathematical optimization in mind. In the model
below, the hyperparameters are set to produce a low perplexity score with 29 topics. As it happens, this model also
has a very high coherence score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hyperoptimized_model</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
<span class="n">hyperoptimized_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">coherence</span> <span class="o">=</span> <span class="n">Coherence</span><span class="p">(</span><span class="n">hyperoptimized_model</span><span class="p">,</span> <span class="n">coherence</span> <span class="o">=</span> <span class="s1">&#39;c_v&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Perplexity score: </span><span class="si">{</span><span class="n">hyperoptimized_model</span><span class="o">.</span><span class="n">perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coherence score: </span><span class="si">{</span><span class="n">coherence</span><span class="o">.</span><span class="n">get_score</span><span class="p">()</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Perplexity score: 6750.9571 
Coherence score: 0.9083
</pre></div>
</div>
</div>
</div>
<p>Those are good looking scores, but look at the topics they produce:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_topic_words</span><span class="p">(</span><span class="n">hyperoptimized_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ irrigators (0.0001%), halitosis (0.0001%), imitative (0.0001%), ranknational (0.0001%), dishonor (0.0001%)
Topic #1: 
+ sax (0.0001%), sportsreality (0.0001%), boogeymani (0.0001%), entrepreneurship (0.0001%), mathers (0.0001%)
Topic #2: 
+ succulent (0.0001%), toast (0.0001%), life (0.0001%), reunió (0.0001%), sueño (0.0001%)
Topic #3: 
+ new (0.0001%), book (0.0001%), life (0.0001%), time (0.0001%), world (0.0001%)
Topic #4: 
+ new (0.0056%), book (0.0053%), life (0.0050%), time (0.0039%), world (0.0037%)
Topic #5: 
+ languish (0.0001%), hypoglycemia (0.0001%), traced (0.0001%), romagna (0.0001%), heavenly (0.0001%)
Topic #6: 
+ palasts (0.0001%), puzzler (0.0001%), supersede (0.0001%), indicates (0.0001%), backbone (0.0001%)
Topic #7: 
+ slaveholder (0.0001%), kazuhiko (0.0001%), bunniculalooks (0.0001%), calliope (0.0001%), tehranthe (0.0001%)
Topic #8: 
+ burr (0.0001%), albino (0.0001%), annointed (0.0001%), conceivable (0.0001%), yates (0.0001%)
Topic #9: 
+ writermark (0.0001%), writes (0.0001%), ancestor (0.0001%), license (0.0001%), world (0.0001%)
Topic #10: 
+ cavalry (0.0001%), book (0.0001%), life (0.0001%), time (0.0001%), world (0.0001%)
Topic #11: 
+ meacham (0.0001%), mccues (0.0001%), antinomian (0.0001%), houseask (0.0001%), onand (0.0001%)
Topic #12: 
+ zulmira (0.0001%), cagliostros (0.0001%), gawkward (0.0001%), violate (0.0001%), packard (0.0001%)
Topic #13: 
+ friendinto (0.0001%), stupidity (0.0001%), foolishness (0.0001%), baldwin (0.0001%), errand (0.0001%)
Topic #14: 
+ heisler (0.0001%), baritone (0.0001%), tenor (0.0001%), trombone (0.0001%), baileymike (0.0001%)
Topic #15: 
+ nevadas (0.0001%), divisadero (0.0001%), carmelite (0.0001%), seriously (0.0001%), philosophical (0.0001%)
Topic #16: 
+ blutchs (0.0001%), nunezs (0.0001%), newfangled (0.0001%), fantasía (0.0001%), pandemic (0.0001%)
Topic #17: 
+ pero (0.0001%), copenhagen (0.0001%), rabble (0.0001%), plumbing (0.0001%), jess (0.0001%)
Topic #18: 
+ travels (0.0001%), altman (0.0001%), palate (0.0001%), maxwell (0.0001%), sophisticate (0.0001%)
Topic #19: 
+ elendel (0.0001%), mistborn (0.0001%), más (0.0001%), cuenca (0.0001%), del (0.0001%)
Topic #20: 
+ denial (0.0001%), aquitaine (0.0001%), sorel (0.0001%), breaks (0.0001%), nightmarish (0.0001%)
Topic #21: 
+ new (0.0001%), book (0.0001%), life (0.0001%), time (0.0001%), world (0.0001%)
Topic #22: 
+ florentine (0.0001%), althussers (0.0001%), lrrp (0.0001%), justamente (0.0001%), ganado (0.0001%)
Topic #23: 
+ moose (0.0001%), outnumber (0.0001%), occupy (0.0001%), ragtag (0.0001%), helicopter (0.0001%)
Topic #24: 
+ covent (0.0001%), pushy (0.0001%), applied (0.0001%), ballad (0.0001%), bungalow (0.0001%)
Topic #25: 
+ new (0.0001%), book (0.0001%), life (0.0001%), time (0.0001%), world (0.0001%)
Topic #26: 
+ disclose (0.0001%), swoon (0.0001%), alleviation (0.0001%), thespray (0.0001%), misgiving (0.0001%)
Topic #27: 
+ new (0.0001%), book (0.0001%), life (0.0001%), time (0.0001%), world (0.0001%)
Topic #28: 
+ installationsand (0.0001%), librarians (0.0001%), overthinking (0.0001%), eli (0.0001%), spill (0.0001%)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_topic_proportions</span><span class="p">(</span><span class="n">hyperoptimized_model</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;Hyperoptimized Model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_topic-modeling_80_0.png" src="_images/06_topic-modeling_80_0.png" />
</div>
</div>
<p>Pretty rotten! One topic all but completely dominates the topic distribution and the others are, well, strange.
They seem to be registering highly specific words within certain blurbs, almost at random.</p>
<p>The challenge of setting your hyperparameters, then, is that it’s a balancing act. In light of the above output,
for example, you might decide to favor interpretability above everything else. But doing so can lead you to
overfitting your model. A model built on the principle of interpretability alone might only ever show you what you
expect to find. Hence the balancing act: the whole process of fine tuning involves incorporating a number of
different considerations (and compromises!) that, at the end of the day, should always work in the service of your
research question – that is, the very reason you want to build a model in the first place!</p>
<p>To return to the question of our own corpus, we won’t change much in our hyperparameters. Again, blurbs are a
fairly constrained genre, so what we’ll do is slightly roll off the alpha value. This will help emphasize specific
document-topic connections, which may help us understand some of the different document types in the corpus. We’ll
stick with the 29 topic model, as it strikes a good balance between perplexity/coherence scoring and general spread
among the topics themselves. As for eta, we’ll leave it alone.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tuned</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">29</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.08</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">357</span><span class="p">)</span>
<span class="n">tuned</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">coherence_model</span> <span class="o">=</span> <span class="n">Coherence</span><span class="p">(</span><span class="n">tuned</span><span class="p">,</span> <span class="n">coherence</span> <span class="o">=</span> <span class="s1">&#39;c_v&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Perplexity score: </span><span class="si">{</span><span class="n">tuned</span><span class="o">.</span><span class="n">perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coherence score: </span><span class="si">{</span><span class="n">coherence_model</span><span class="o">.</span><span class="n">get_score</span><span class="p">()</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Perplexity score: 9867.0200 
Coherence score: 0.6443
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_topic_words</span><span class="p">(</span><span class="n">tuned</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0: 
+ home (0.0305%), cat (0.0217%), friend (0.0179%), dog (0.0156%), city (0.0137%)
Topic #1: 
+ new (0.0397%), time (0.0337%), book (0.0335%), york (0.0307%), award (0.0179%)
Topic #2: 
+ woman (0.0549%), life (0.0210%), relationship (0.0191%), mother (0.0171%), husband (0.0171%)
Topic #3: 
+ new (0.0268%), just (0.0194%), time (0.0182%), make (0.0139%), like (0.0121%)
Topic #4: 
+ life (0.0245%), spiritual (0.0196%), live (0.0192%), art (0.0159%), practice (0.0139%)
Topic #5: 
+ vampire (0.0162%), financial (0.0149%), rule (0.0128%), money (0.0128%), crisis (0.0081%)
Topic #6: 
+ new (0.0271%), man (0.0143%), magic (0.0138%), tale (0.0133%), return (0.0130%)
Topic #7: 
+ book (0.0164%), use (0.0128%), learn (0.0116%), show (0.0108%), offer (0.0101%)
Topic #8: 
+ classic (0.0284%), work (0.0241%), penguin (0.0228%), text (0.0172%), year (0.0172%)
Topic #9: 
+ life (0.0333%), story (0.0247%), world (0.0184%), year (0.0172%), man (0.0110%)
Topic #10: 
+ music (0.0242%), star (0.0173%), sport (0.0173%), lego (0.0169%), team (0.0160%)
Topic #11: 
+ recipe (0.0296%), food (0.0231%), drink (0.0126%), italian (0.0126%), cook (0.0126%)
Topic #12: 
+ japanese (0.0228%), word (0.0140%), jean (0.0127%), whale (0.0095%), landscape (0.0089%)
Topic #13: 
+ guide (0.0291%), new (0.0177%), travel (0.0158%), include (0.0136%), create (0.0107%)
Topic #14: 
+ tale (0.0245%), art (0.0230%), king (0.0204%), paint (0.0174%), artist (0.0158%)
Topic #15: 
+ century (0.0164%), french (0.0136%), war (0.0128%), draw (0.0120%), europe (0.0112%)
Topic #16: 
+ health (0.0189%), body (0.0145%), weight (0.0123%), food (0.0116%), program (0.0113%)
Topic #17: 
+ book (0.0425%), child (0.0204%), reader (0.0178%), little (0.0171%), animal (0.0169%)
Topic #18: 
+ collection (0.0218%), book (0.0216%), volume (0.0183%), story (0.0166%), include (0.0164%)
Topic #19: 
+ god (0.0568%), religious (0.0174%), jesus (0.0146%), faith (0.0141%), bible (0.0136%)
Topic #20: 
+ war (0.0300%), battle (0.0152%), force (0.0137%), ship (0.0127%), star (0.0122%)
Topic #21: 
+ christmas (0.0256%), little (0.0156%), holiday (0.0139%), come (0.0135%), baby (0.0126%)
Topic #22: 
+ social (0.0121%), theory (0.0121%), political (0.0114%), public (0.0101%), analysis (0.0098%)
Topic #23: 
+ american (0.0240%), history (0.0216%), new (0.0171%), world (0.0151%), time (0.0131%)
Topic #24: 
+ love (0.0400%), life (0.0194%), family (0.0172%), young (0.0161%), old (0.0135%)
Topic #25: 
+ mystery (0.0289%), town (0.0224%), murder (0.0211%), police (0.0149%), detective (0.0130%)
Topic #26: 
+ man (0.0225%), woman (0.0225%), romance (0.0162%), passion (0.0135%), fall (0.0106%)
Topic #27: 
+ book (0.0371%), character (0.0297%), series (0.0163%), fan (0.0150%), action (0.0144%)
Topic #28: 
+ secret (0.0187%), world (0.0147%), dark (0.0134%), know (0.0116%), face (0.0110%)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_topic_proportions</span><span class="p">(</span><span class="n">tuned</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;Fine-Tuned Model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_topic-modeling_84_0.png" src="_images/06_topic-modeling_84_0.png" />
</div>
</div>
<p>Now, even with a fine-tuned model there are likely to be some inscrutable topics. Look at topic 12 for example:
what in the world does that represent? Topic 0 has a few less nonsequitors, but from this vantage it’s hard to
discern what it might be about. This, of course, is part of the point of topic modeling: it’s a way of helping us
discover new things about a corpus. Are topics 0 and 12 nonsense topics, or do they have some inner coherence? With
our model built, it’s on us to start exploring.</p>
</div>
</div>
<div class="section" id="exploring-a-model">
<h2><span class="section-number">6.6. </span>Exploring a Model<a class="headerlink" href="#exploring-a-model" title="Permalink to this headline">¶</a></h2>
<p>In this final section, we’ll do just that. It’s often helpful to visualize topic mdoels to aid us in our
explorations, so we’ll make a few of these and see what we can find.</p>
<p>Before doing so, let’s rebuild a theta from the fine-tuned model. Remember that a theta is a document-topic matrix,
where each cell is a probability score for a document’s association with a particular topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_topic_distributions</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tuned</span><span class="o">.</span><span class="n">docs</span><span class="p">]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_topic_distributions</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">/=</span> <span class="n">theta</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll also make a quick set of labels for our topics, which list the top five words for each one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tuned</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">tuned</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">top_n</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;#</span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">: &quot;</span> <span class="o">+</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
    <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A heatmap is a natural fit for inspecting these probability distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">random_titles</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">selected_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">random_titles</span><span class="p">)]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">selected_theta</span><span class="p">,</span>
                 <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">,</span>
                 <span class="n">linewidths</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">linecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span>
                 <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;Blues&#39;</span>
                <span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Topic&#39;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Title&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">&#39;top&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_top</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="s1">&#39;left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_topic-modeling_91_0.png" src="_images/06_topic-modeling_91_0.png" />
</div>
</div>
<p>Topic 8 seems to be about the classics. What kind of titles have a particularly high association with this topic?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_8</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">8</span><span class="p">][</span><span class="mi">8</span><span class="p">]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">topic_8</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">selected</span><span class="p">[[</span><span class="s1">&#39;AUTHOR&#39;</span><span class="p">,</span> <span class="s1">&#39;TITLE&#39;</span><span class="p">,</span> <span class="s1">&#39;GENRE&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">TOPIC_8_SCORE</span> <span class="o">=</span> <span class="n">topic_8</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">selected</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">hide_index</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style  type="text/css" >
</style><table id="T_df0c3_" ><thead>    <tr>        <th class="col_heading level0 col0" >AUTHOR</th>        <th class="col_heading level0 col1" >TITLE</th>        <th class="col_heading level0 col2" >GENRE</th>        <th class="col_heading level0 col3" >TOPIC_8_SCORE</th>    </tr></thead><tbody>
                <tr>
                                <td id="T_df0c3_row0_col0" class="data row0 col0" >Horatio Alger</td>
                        <td id="T_df0c3_row0_col1" class="data row0 col1" >Ragged Dick and Struggling Upward</td>
                        <td id="T_df0c3_row0_col2" class="data row0 col2" >Classics</td>
                        <td id="T_df0c3_row0_col3" class="data row0 col3" >0.553598</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row1_col0" class="data row1 col0" >Stan Berenstain, Jan Berenstain</td>
                        <td id="T_df0c3_row1_col1" class="data row1 col1" >The Berenstain Bears Blaze a Trail</td>
                        <td id="T_df0c3_row1_col2" class="data row1 col2" >Children’s Books</td>
                        <td id="T_df0c3_row1_col3" class="data row1 col3" >0.229238</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row2_col0" class="data row2 col0" >Chretien de Troyes</td>
                        <td id="T_df0c3_row2_col1" class="data row2 col1" >Arthurian Romances</td>
                        <td id="T_df0c3_row2_col2" class="data row2 col2" >Classics</td>
                        <td id="T_df0c3_row2_col3" class="data row2 col3" >0.781303</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row3_col0" class="data row3 col0" >Nicola Sacco, Bartolomeo Vanzetti</td>
                        <td id="T_df0c3_row3_col1" class="data row3 col1" >The Letters of Sacco and Vanzetti</td>
                        <td id="T_df0c3_row3_col2" class="data row3 col2" >Classics</td>
                        <td id="T_df0c3_row3_col3" class="data row3 col3" >0.542920</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row4_col0" class="data row4 col0" >Elizabeth Winthrop</td>
                        <td id="T_df0c3_row4_col1" class="data row4 col1" >The Battle for the Castle</td>
                        <td id="T_df0c3_row4_col2" class="data row4 col2" >Children’s Books</td>
                        <td id="T_df0c3_row4_col3" class="data row4 col3" >0.229807</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row5_col0" class="data row5 col0" >Leslie Alcock</td>
                        <td id="T_df0c3_row5_col1" class="data row5 col1" >Arthur's Britain</td>
                        <td id="T_df0c3_row5_col2" class="data row5 col2" >Nonfiction</td>
                        <td id="T_df0c3_row5_col3" class="data row5 col3" >0.146158</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row6_col0" class="data row6 col0" >Frances Hodgson Burnett</td>
                        <td id="T_df0c3_row6_col1" class="data row6 col1" >A Little Princess</td>
                        <td id="T_df0c3_row6_col2" class="data row6 col2" >Classics</td>
                        <td id="T_df0c3_row6_col3" class="data row6 col3" >0.250944</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row7_col0" class="data row7 col0" >Mary Boykin Chesnut</td>
                        <td id="T_df0c3_row7_col1" class="data row7 col1" >Mary Chesnut's Diary</td>
                        <td id="T_df0c3_row7_col2" class="data row7 col2" >Classics</td>
                        <td id="T_df0c3_row7_col3" class="data row7 col3" >0.572314</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row8_col0" class="data row8 col0" >Kenneth Grahame</td>
                        <td id="T_df0c3_row8_col1" class="data row8 col1" >The Wind in the Willows</td>
                        <td id="T_df0c3_row8_col2" class="data row8 col2" >Classics</td>
                        <td id="T_df0c3_row8_col3" class="data row8 col3" >0.608761</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row9_col0" class="data row9 col0" >Harriet Jacobs</td>
                        <td id="T_df0c3_row9_col1" class="data row9 col1" >Incidents in the Life of a Slave Girl</td>
                        <td id="T_df0c3_row9_col2" class="data row9 col2" >Classics</td>
                        <td id="T_df0c3_row9_col3" class="data row9 col3" >0.575178</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row10_col0" class="data row10 col0" >Richard E. Kim</td>
                        <td id="T_df0c3_row10_col1" class="data row10 col1" >The Martyred</td>
                        <td id="T_df0c3_row10_col2" class="data row10 col2" >Classics</td>
                        <td id="T_df0c3_row10_col3" class="data row10 col3" >0.478683</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row11_col0" class="data row11 col0" >Friedrich Nietzsche</td>
                        <td id="T_df0c3_row11_col1" class="data row11 col1" >The Will to Power</td>
                        <td id="T_df0c3_row11_col2" class="data row11 col2" >Nonfiction</td>
                        <td id="T_df0c3_row11_col3" class="data row11 col3" >0.290242</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row12_col0" class="data row12 col0" >Various</td>
                        <td id="T_df0c3_row12_col1" class="data row12 col1" >The Penguin Book of Modern African Poetry</td>
                        <td id="T_df0c3_row12_col2" class="data row12 col2" >Classics</td>
                        <td id="T_df0c3_row12_col3" class="data row12 col3" >0.436538</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row13_col0" class="data row13 col0" >Various</td>
                        <td id="T_df0c3_row13_col1" class="data row13 col1" >The Portable Twentieth-Century Russian Reader</td>
                        <td id="T_df0c3_row13_col2" class="data row13 col2" >Classics</td>
                        <td id="T_df0c3_row13_col3" class="data row13 col3" >0.681862</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row14_col0" class="data row14 col0" >Ron Roy</td>
                        <td id="T_df0c3_row14_col1" class="data row14 col1" >A to Z Mysteries: The Lucky Lottery</td>
                        <td id="T_df0c3_row14_col2" class="data row14 col2" >Children’s Books</td>
                        <td id="T_df0c3_row14_col3" class="data row14 col3" >0.211067</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row15_col0" class="data row15 col0" >Anonymous</td>
                        <td id="T_df0c3_row15_col1" class="data row15 col1" >The Vinland Sagas</td>
                        <td id="T_df0c3_row15_col2" class="data row15 col2" >Classics</td>
                        <td id="T_df0c3_row15_col3" class="data row15 col3" >0.628127</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row16_col0" class="data row16 col0" >Anonymous</td>
                        <td id="T_df0c3_row16_col1" class="data row16 col1" >The Cloud of Unknowing and Other Works</td>
                        <td id="T_df0c3_row16_col2" class="data row16 col2" >Classics</td>
                        <td id="T_df0c3_row16_col3" class="data row16 col3" >0.496929</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row17_col0" class="data row17 col0" >Thomas Hardy</td>
                        <td id="T_df0c3_row17_col1" class="data row17 col1" >The Distracted Preacher</td>
                        <td id="T_df0c3_row17_col2" class="data row17 col2" >Fiction</td>
                        <td id="T_df0c3_row17_col3" class="data row17 col3" >0.272698</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row18_col0" class="data row18 col0" >Arthur Miller</td>
                        <td id="T_df0c3_row18_col1" class="data row18 col1" >A View from the Bridge</td>
                        <td id="T_df0c3_row18_col2" class="data row18 col2" >Classics</td>
                        <td id="T_df0c3_row18_col3" class="data row18 col3" >0.630500</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row19_col0" class="data row19 col0" >William Shakespeare</td>
                        <td id="T_df0c3_row19_col1" class="data row19 col1" >King John</td>
                        <td id="T_df0c3_row19_col2" class="data row19 col2" >Classics</td>
                        <td id="T_df0c3_row19_col3" class="data row19 col3" >0.604255</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row20_col0" class="data row20 col0" >Various</td>
                        <td id="T_df0c3_row20_col1" class="data row20 col1" >Hippocratic Writings</td>
                        <td id="T_df0c3_row20_col2" class="data row20 col2" >Classics</td>
                        <td id="T_df0c3_row20_col3" class="data row20 col3" >0.563660</td>
            </tr>
            <tr>
                                <td id="T_df0c3_row21_col0" class="data row21 col0" >Abbe Prevost</td>
                        <td id="T_df0c3_row21_col1" class="data row21 col1" >Manon Lescaut</td>
                        <td id="T_df0c3_row21_col2" class="data row21 col2" >Classics</td>
                        <td id="T_df0c3_row21_col3" class="data row21 col3" >0.570990</td>
            </tr>
    </tbody></table></div></div>
</div>
<p>Looks about right, though it’s interesting to see a few children’s books here. Were we to explore this a bit
further, we’d probably want to take a look at those probability scores. The seeming outliers in this listing also
tend to have a low score for the topic, so it would be worth investigating whether their other topic associations
compete with this top score.</p>
<p>Certain topics seem to map very nicely onto particular book genres. Here are some titles associated with topic 11,
which seems to be about recipes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_11</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">11</span><span class="p">][</span><span class="mi">11</span><span class="p">]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">topic_11</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">selected</span><span class="p">[[</span><span class="s1">&#39;AUTHOR&#39;</span><span class="p">,</span> <span class="s1">&#39;TITLE&#39;</span><span class="p">,</span> <span class="s1">&#39;GENRE&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">TOPIC_11_SCORE</span> <span class="o">=</span> <span class="n">topic_11</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">selected</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">hide_index</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style  type="text/css" >
</style><table id="T_32940_" ><thead>    <tr>        <th class="col_heading level0 col0" >AUTHOR</th>        <th class="col_heading level0 col1" >TITLE</th>        <th class="col_heading level0 col2" >GENRE</th>        <th class="col_heading level0 col3" >TOPIC_11_SCORE</th>    </tr></thead><tbody>
                <tr>
                                <td id="T_32940_row0_col0" class="data row0 col0" >Jeffrey Steingarten</td>
                        <td id="T_32940_row0_col1" class="data row0 col1" >The Man Who Ate Everything</td>
                        <td id="T_32940_row0_col2" class="data row0 col2" >Nonfiction</td>
                        <td id="T_32940_row0_col3" class="data row0 col3" >0.279978</td>
            </tr>
            <tr>
                                <td id="T_32940_row1_col0" class="data row1 col0" >Suvir Saran, Stephanie Lyness</td>
                        <td id="T_32940_row1_col1" class="data row1 col1" >Indian Home Cooking</td>
                        <td id="T_32940_row1_col2" class="data row1 col2" >Nonfiction</td>
                        <td id="T_32940_row1_col3" class="data row1 col3" >0.741052</td>
            </tr>
            <tr>
                                <td id="T_32940_row2_col0" class="data row2 col0" >Dominique Fabre</td>
                        <td id="T_32940_row2_col1" class="data row2 col1" >The Waitress Was New</td>
                        <td id="T_32940_row2_col2" class="data row2 col2" >Fiction</td>
                        <td id="T_32940_row2_col3" class="data row2 col3" >0.225956</td>
            </tr>
            <tr>
                                <td id="T_32940_row3_col0" class="data row3 col0" >Susan Jane White</td>
                        <td id="T_32940_row3_col1" class="data row3 col1" >The Virtuous Tart</td>
                        <td id="T_32940_row3_col2" class="data row3 col2" >Nonfiction</td>
                        <td id="T_32940_row3_col3" class="data row3 col3" >0.287522</td>
            </tr>
            <tr>
                                <td id="T_32940_row4_col0" class="data row4 col0" >Nicola Graimes</td>
                        <td id="T_32940_row4_col1" class="data row4 col1" >The Low-Sugar Cookbook</td>
                        <td id="T_32940_row4_col2" class="data row4 col2" >Nonfiction</td>
                        <td id="T_32940_row4_col3" class="data row4 col3" >0.371184</td>
            </tr>
            <tr>
                                <td id="T_32940_row5_col0" class="data row5 col0" >Hi Soo Shin Hepinstall</td>
                        <td id="T_32940_row5_col1" class="data row5 col1" >Growing up in a Korean Kitchen</td>
                        <td id="T_32940_row5_col2" class="data row5 col2" >Nonfiction</td>
                        <td id="T_32940_row5_col3" class="data row5 col3" >0.390505</td>
            </tr>
            <tr>
                                <td id="T_32940_row6_col0" class="data row6 col0" >Dana Shultz</td>
                        <td id="T_32940_row6_col1" class="data row6 col1" >Minimalist Baker's Everyday Cooking</td>
                        <td id="T_32940_row6_col2" class="data row6 col2" >Nonfiction</td>
                        <td id="T_32940_row6_col3" class="data row6 col3" >0.591667</td>
            </tr>
            <tr>
                                <td id="T_32940_row7_col0" class="data row7 col0" >Rosie Genova</td>
                        <td id="T_32940_row7_col1" class="data row7 col1" >Murder and Marinara</td>
                        <td id="T_32940_row7_col2" class="data row7 col2" >Fiction</td>
                        <td id="T_32940_row7_col3" class="data row7 col3" >0.282058</td>
            </tr>
            <tr>
                                <td id="T_32940_row8_col0" class="data row8 col0" >Jean-Pierre Moullé, Denise Lurton Moullé</td>
                        <td id="T_32940_row8_col1" class="data row8 col1" >French Roots</td>
                        <td id="T_32940_row8_col2" class="data row8 col2" >Nonfiction</td>
                        <td id="T_32940_row8_col3" class="data row8 col3" >0.492593</td>
            </tr>
            <tr>
                                <td id="T_32940_row9_col0" class="data row9 col0" >Sarah Schlesinger</td>
                        <td id="T_32940_row9_col1" class="data row9 col1" >500 Low-Fat Fruit and Vegetable Recipes</td>
                        <td id="T_32940_row9_col2" class="data row9 col2" >Nonfiction</td>
                        <td id="T_32940_row9_col3" class="data row9 col3" >0.623170</td>
            </tr>
    </tbody></table></div></div>
</div>
<p>And here are a few titles associated with topic 19, which has a lot of religious texts associated with it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_19</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">19</span><span class="p">][</span><span class="mi">19</span><span class="p">]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">[</span><span class="n">manifest</span><span class="p">[</span><span class="s1">&#39;TITLE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">topic_19</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span>
<span class="n">selected</span> <span class="o">=</span> <span class="n">selected</span><span class="p">[[</span><span class="s1">&#39;AUTHOR&#39;</span><span class="p">,</span> <span class="s1">&#39;TITLE&#39;</span><span class="p">,</span> <span class="s1">&#39;GENRE&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">TOPIC_19_SCORE</span> <span class="o">=</span> <span class="n">topic_19</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">selected</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">hide_index</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style  type="text/css" >
</style><table id="T_d0711_" ><thead>    <tr>        <th class="col_heading level0 col0" >AUTHOR</th>        <th class="col_heading level0 col1" >TITLE</th>        <th class="col_heading level0 col2" >GENRE</th>        <th class="col_heading level0 col3" >TOPIC_19_SCORE</th>    </tr></thead><tbody>
                <tr>
                                <td id="T_d0711_row0_col0" class="data row0 col0" >Jack Miles</td>
                        <td id="T_d0711_row0_col1" class="data row0 col1" >God: A Biography</td>
                        <td id="T_d0711_row0_col2" class="data row0 col2" >Nonfiction</td>
                        <td id="T_d0711_row0_col3" class="data row0 col3" >0.322673</td>
            </tr>
            <tr>
                                <td id="T_d0711_row1_col0" class="data row1 col0" >Phyllis Tickle</td>
                        <td id="T_d0711_row1_col1" class="data row1 col1" >The Divine Hours (Volume Three): Prayers for Springtime</td>
                        <td id="T_d0711_row1_col2" class="data row1 col2" >Nonfiction</td>
                        <td id="T_d0711_row1_col3" class="data row1 col3" >0.203675</td>
            </tr>
            <tr>
                                <td id="T_d0711_row2_col0" class="data row2 col0" >Patricia Sprinkle</td>
                        <td id="T_d0711_row2_col1" class="data row2 col1" >Friday's Daughter</td>
                        <td id="T_d0711_row2_col2" class="data row2 col2" >Fiction</td>
                        <td id="T_d0711_row2_col3" class="data row2 col3" >0.257325</td>
            </tr>
            <tr>
                                <td id="T_d0711_row3_col0" class="data row3 col0" >Kay Arthur</td>
                        <td id="T_d0711_row3_col1" class="data row3 col1" >Our Covenant God</td>
                        <td id="T_d0711_row3_col2" class="data row3 col2" >Nonfiction</td>
                        <td id="T_d0711_row3_col3" class="data row3 col3" >0.372332</td>
            </tr>
            <tr>
                                <td id="T_d0711_row4_col0" class="data row4 col0" >Nakaba Suzuki</td>
                        <td id="T_d0711_row4_col1" class="data row4 col1" >The Seven Deadly Sins 4</td>
                        <td id="T_d0711_row4_col2" class="data row4 col2" >Fiction</td>
                        <td id="T_d0711_row4_col3" class="data row4 col3" >0.304005</td>
            </tr>
            <tr>
                                <td id="T_d0711_row5_col0" class="data row5 col0" >Geza Vermes</td>
                        <td id="T_d0711_row5_col1" class="data row5 col1" >The Changing Faces of Jesus</td>
                        <td id="T_d0711_row5_col2" class="data row5 col2" >Nonfiction</td>
                        <td id="T_d0711_row5_col3" class="data row5 col3" >0.294294</td>
            </tr>
            <tr>
                                <td id="T_d0711_row6_col0" class="data row6 col0" >Garry Wills</td>
                        <td id="T_d0711_row6_col1" class="data row6 col1" >Papal Sin</td>
                        <td id="T_d0711_row6_col2" class="data row6 col2" >Nonfiction</td>
                        <td id="T_d0711_row6_col3" class="data row6 col3" >0.438749</td>
            </tr>
            <tr>
                                <td id="T_d0711_row7_col0" class="data row7 col0" >Joni Eareckson Tada</td>
                        <td id="T_d0711_row7_col1" class="data row7 col1" >Glorious Intruder</td>
                        <td id="T_d0711_row7_col2" class="data row7 col2" >Nonfiction</td>
                        <td id="T_d0711_row7_col3" class="data row7 col3" >0.186482</td>
            </tr>
            <tr>
                                <td id="T_d0711_row8_col0" class="data row8 col0" >Tony Evans</td>
                        <td id="T_d0711_row8_col1" class="data row8 col1" >Dry Bones Dancing</td>
                        <td id="T_d0711_row8_col2" class="data row8 col2" >Nonfiction</td>
                        <td id="T_d0711_row8_col3" class="data row8 col3" >0.357153</td>
            </tr>
            <tr>
                                <td id="T_d0711_row9_col0" class="data row9 col0" >Jean-Pierre Isbouts</td>
                        <td id="T_d0711_row9_col1" class="data row9 col1" >In the Footsteps of Jesus</td>
                        <td id="T_d0711_row9_col2" class="data row9 col2" >Nonfiction</td>
                        <td id="T_d0711_row9_col3" class="data row9 col3" >0.289918</td>
            </tr>
    </tbody></table></div></div>
</div>
<p>Notice however that you might see contrasting views within these outputs. <em>The Flying Spaghetti Monster</em> is in this
list of “religious” texts, alongside books like <em>God: A Biography</em> or <em>Papal Sin</em>. Topic models, remember, are
ultimately counting token co-occurences, not the different semantic valences of a word – it’s up to use to parse
the latter. This becomes especially important, for example, when we see overlapping words in topics, as in topics
10 and 20, where “star” is in the top words for both. Sometimes this can indicate a difference in semantics, other
times it may indeed be the same sense of the word appearing multiple times.</p>
<p>To parse this difference, it’s helpful to examine the overall similarities and differences between topics, much in
the way we projected our documents into a vector space in the previous chapter. We’ll prepare the code to do
something similar here but will save the final result for a separate webpage. Below, we’ll produce the following:</p>
<ul class="simple">
<li><p>A topic-term distribution matrix (word probabilities for each topic)</p></li>
<li><p>The lengths of every document</p></li>
<li><p>A list of the corpus vocabulary</p></li>
<li><p>The corresponding frequency counts for the corpus vocabulary</p></li>
</ul>
<p>Once we’ve made these, we’ll prepare our visualization data with a package called <code class="docutils literal notranslate"><span class="pre">pyLDAvis</span></code> and save it.</p>
<div class="margin sidebar">
<p class="sidebar-title">We’re saving this, but…</p>
<p>…you could also use <code class="docutils literal notranslate"><span class="pre">pyLDAvis(vis_data)</span></code> to show it in your own notebook (it just doesn’t play nicely with our
reader format).</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyLDAvis</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="n">topic_terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tuned</span><span class="o">.</span><span class="n">get_topic_word_dist</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tuned</span><span class="o">.</span><span class="n">k</span><span class="p">)])</span>
<span class="n">doc_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tuned</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tuned</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span>
<span class="n">term_frequency</span> <span class="o">=</span> <span class="n">tuned</span><span class="o">.</span><span class="n">used_vocab_freq</span>

<span class="n">vis_data</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">topic_terms</span><span class="p">,</span>
                            <span class="n">theta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                            <span class="n">doc_lengths</span><span class="p">,</span>
                            <span class="n">vocab</span><span class="p">,</span>
                            <span class="n">term_frequency</span><span class="p">,</span>
                            <span class="n">start_index</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="n">sort_topics</span> <span class="o">=</span> <span class="kc">False</span>
                           <span class="p">)</span>

<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">vis_data</span><span class="p">,</span> <span class="s2">&quot;data/session_three/output/topic_model_plot.html&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With that done, we’ve finished our initial work with topic models – though the work of exploration has just begun!
You can find the resultant visualization of the above <a href="./topic_model_plot.html">here</a>. It’s a
scatter plot that represents (roughly) topic similarity; the size of each topic circle corresponds to that topic’s
proportion in the model. Play around with it a little bit to see what you can find!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "nlp"
        },
        kernelOptions: {
            kernelName: "nlp",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'nlp'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="05_clustering-and-classification.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">5. </span>Clustering and Classification</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Tyler Shoemaker and Carl Stahmer<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>