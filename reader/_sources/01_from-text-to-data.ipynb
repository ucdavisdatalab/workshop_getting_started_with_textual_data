{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed95787f",
   "metadata": {},
   "source": [
    "From Text to Data\n",
    "==============\n",
    "\n",
    "This chapter, which we've asked you to read prior to the first workshop, is a general discussion of working with \n",
    "textual data in Python. While the workshop series assumes you have at least a basic understanding of Python, we'll \n",
    "quickly review how to load, or \"read in,\" a single text file and format it for text analysis. We'll do so both as a \n",
    "refresher and because this simple action illuminates an important aspect of working with textual data: namely, that \n",
    "to your computer, text is above all a **sequence of characters**. This is a key thing to keep in mind when \n",
    "preparing your data for text mining and/or NLP.\n",
    "\n",
    "As you work through this chapter, use it as a check on your Python skills. If you feel comfortable writing the code \n",
    "below, you should be prepared for our sessions. The skills covered in this chapter include:\n",
    "\n",
    "+ Loading text data into Python\n",
    "+ Working with different Python data structures (strings, lists, dictionaries)\n",
    "+ Control flow with `for` loops\n",
    "+ Using `Pandas` dataframes\n",
    "\n",
    "```{tip}\n",
    "Need to brush up on Python? The DataLab offers a Python Basics workshop series. You can find links to the series \n",
    "reader and recording on our [Workshop Archive page].\n",
    "\n",
    "[Workshop Archive page]: https://datalab.ucdavis.edu/workshops/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858c26a",
   "metadata": {},
   "source": [
    "Loading Text\n",
    "---------------\n",
    "\n",
    "To open a text file, we'll use `with...open`. This saves us from forgetting to close the file stream and thereby \n",
    "frees up a little memory for later computation. The memory strain a single text file puts on your computer isn't \n",
    "very large at all, but dozens, to say nothing of hundreds or thousands of texts, can start to slow things down, so \n",
    "it's good to get in the habit of automatically closing file streams right from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shelley_frankenstein.txt\", 'r') as f:\n",
    "    frankenstein = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be96d10",
   "metadata": {},
   "source": [
    "### Plain text\n",
    "\n",
    "Here we use `r` in the `mode` argument because we're working with **plain text** data (as opposed to **binary** \n",
    "data, which would require `rb`). In computing, plain text has multiple fuzzy, interlocking meanings, but generally \n",
    "it refers to some kind of data that is stored in a human-readable form, which is to say, it is comprised of a \n",
    "collection of text characters (usually [ASCII], but increasingly [UTF-8]).\n",
    "\n",
    "[ASCII]: https://en.wikipedia.org/wiki/ASCII\n",
    "[UTF-8]: https://en.wikipedia.org/wiki/UTF-8\n",
    "\n",
    "All the texts we'll be working with are plain text files. But depending on your research area, access to plain text \n",
    "representations of documents may be the exception, not the rule. If that's the case, you would need to convert \n",
    "your documents into a machine-readable form. Options for doing so range from automated methods, like [optical \n",
    "character recognition], to good old fashioned hand transcription.\n",
    "\n",
    "[optical character recognition]: https://en.wikipedia.org/wiki/Optical_character_recognition\n",
    "\n",
    "In plain text representations, every keystroke you would use to hand-transcribe a text has a corresponding sequence \n",
    "of characters. That means this print output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a373a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frankenstein[:364])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e871f",
   "metadata": {},
   "source": [
    "...is represented by the following in plain text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58195e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frankenstein[:364]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1752cf7",
   "metadata": {},
   "source": [
    "```{margin} You might ask...\n",
    "Why aren't spaces also represented by a character? Well, they are, but even in this relatively unformatted view \n",
    "your computer continues to automatically render text in a human-readable way.\n",
    "```\n",
    "\n",
    "See all the `\\n`, or **newline**, characters? Each one represents a linebreak. On the backend, your computer uses \n",
    "newline characters to demarcate things like paragraphs, stanzas, titles, and so forth, but typically it suppreses \n",
    "these characters when it renders text for our eyes. What we see in the two different outputs above, then, is a \n",
    "difference between **print conventions** and **code conventions**: what appears as a blank in the former is in fact \n",
    "an addressable unit in the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e5fa3",
   "metadata": {},
   "source": [
    "### Character sequences\n",
    "\n",
    "The distinction between print and code conventions has some significant consequences for us, both practical and \n",
    "conceptual. While, in the print view of the world, we tend to think of the word as the atomic unit of text, in the \n",
    "code view of the world, text is -- again -- a **sequence of characters**. We can see this if we try to count how \n",
    "many units are in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of Frankenstein is:\", len(frankenstein))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b0c32",
   "metadata": {},
   "source": [
    "The Penguin edition of _Frankenstein_ clocks in at ~220 pages. If we assume each page contains something like 350 \n",
    "words, that makes the book ~77,000 words long -- far less than the number outputted above. Why, then, did Python \n",
    "output this number? Because it counted characters, not words. To Python, our text is currently represented as a \n",
    "giant blob. This blob makes no distinction between the start of one word and the next; its atomic unit is the \n",
    "character, and so most of the operations we can run on it will thus address characters, not words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd46c61",
   "metadata": {},
   "source": [
    "### Tokenizing strings\n",
    "\n",
    "But we want to address our text at the level of words. To do so, we'll need to manipulate how Python represents our \n",
    "data, changing it from a long stream of characters to discrete (and preferably indexed) units. The process of doing \n",
    "this is called **tokenization**. _To tokenize_ means to break a continuous sequence of text data into substrings, \n",
    "which we call \"tokens.\" Ultimately, tokens are what we will end up counting in text analytics. They are the atomic \n",
    "unit of/for almost everything we'll discuss in this series.\n",
    "\n",
    "Notably, a token is more of a generic entity than it is a particular kind of text. Tokens don't always mean words \n",
    "(though you'll often see them treated this way). In one sense, for example, our text is already tokenized -- it's \n",
    "just tokenized by characters, which isn't much use for us now. What we want to do, then, is tokenize our text in \n",
    "such a way that we can address each word therein. This will help us keep track of those words, rather than mucking \n",
    "around with blobby character data.\n",
    "\n",
    "There are a number of different Python libraries that can tokenize text for you, but it's easy enough to do one \n",
    "version of this task with Python's base functionality. For now, we'll simply use `split()`. The default character \n",
    "this function takes in its argument is any whitespace, which will nicely isolate words (whitespace characters \n",
    "include `\\n`, `\\t`, and of course plain old spaces). We'll call `split()` on our text and save the result to `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f357bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = frankenstein.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fa8d5",
   "metadata": {},
   "source": [
    "Now, if we call `len()` on `doc`, we'll see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of Frankenstein is:\", len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b816002",
   "metadata": {},
   "source": [
    "Much better! Now that we have tokenized by words, this number is considerably closer to our estimations above.\n",
    "\n",
    "```{admonition} A look ahead\n",
    "While you'll most often tokenize on whitespaces, there are cases where you might want to chunk your text using \n",
    "different characters, or even entire sequences of characters. For example, if you are studying poetry, you might \n",
    "want to know some information about the average number of lines in a stanza. In that case, splitting on `\\n` could \n",
    "be more useful than space. We'll cover this topic more fully in a later section; for the moment just keep in mind \n",
    "that there are many different and valid ways to tokenize text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e8b8d",
   "metadata": {},
   "source": [
    "Counting Words\n",
    "-------------------\n",
    "\n",
    "With our text data loaded and properly formatted, we can start one of the core tasks of text analysis: counting words. While the next chapter will discuss this process in greater detail, we'll preview it here to get a sense of \n",
    "what's to come and to review the basics of control flow in Python.\n",
    "\n",
    "Splitting text transforms it into a list, where each word has its own separate index position. Remember that, in \n",
    "Python, lists are ordered arrays that store multiple, potentially repeatable values. With this representation of \n",
    "our data, it's much easier get global word counts using something as simple as a `for` loop: we can simply iterate \n",
    "through every item in the list and tally them all up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f84a2d",
   "metadata": {},
   "source": [
    "### A first pass\n",
    "\n",
    "Let's do this now. We can build a little loop to find the cumulative number of times each word occurs in \n",
    "_Frankenstein_. To store this information, we'll use a dictionary. This will provide us with a way to access the \n",
    "counts of individual words once we've looped through the entire novel.\n",
    "\n",
    "```{margin} What this loop does:\n",
    "For every word in the novel, check whether that word is in the dictionary:\n",
    "+ If it isn't in the dictionary, add that word and count `1`\n",
    "+ If it is, increase that word's count by `1`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for word in doc:\n",
    "    if word not in word_counts:\n",
    "        word_counts[word] = 1\n",
    "    else:\n",
    "        word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab13cf",
   "metadata": {},
   "source": [
    "With this done, we can determine the total number of unique words in _Frankstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of unique words in Frankenstein:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ca043",
   "metadata": {},
   "source": [
    "And we can also access the counts of individual words. Let's pick two: \"imagination\" and \"monster.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [\"imagination\", \"monster\"]:\n",
    "    print(f\"{word:<12} {word_counts[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68097eb6",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "You can use `:<[NUMBER]` in conjunction with string formatting to keep your ouput tidy by controlling the print \n",
    "spacing.\n",
    "```\n",
    "\n",
    "If you're familiar with _Frankenstein_, you'll know that it's an epistolary novel, meaning it's written as a series \n",
    "of letters. It even begins this way: the heading in the first print output above reads \"Letter 1.\"\n",
    "\n",
    "With that in mind, let's tack on \"letter\" to our loop above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [\"imagination\", \"monster\", \"letter\"]:\n",
    "    print(f\"{word:<12} {word_counts[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8dbff",
   "metadata": {},
   "source": [
    "### Top words\n",
    "\n",
    "Great! This all seems to work well, though we won't get very far if we continue to take a top-down approach and \n",
    "spot check single words. How would we know what all is in the novel and what isn't? Instead of approaching the data \n",
    "in this way, it would be more useful to see what turns up if we just look at the count distribution as a whole.\n",
    "\n",
    "To do so, let's sort our dictionary by the number of times each word appears. Putting these counts in a `Pandas` \n",
    "dataframe will make them much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(word_counts, columns = ['COUNT'], orient = 'index')\n",
    "df = df.sort_values('COUNT', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a164fa",
   "metadata": {},
   "source": [
    "Now let's take a look at the 50 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d632c",
   "metadata": {},
   "source": [
    "And there they are!\n",
    "\n",
    "```{warning}\n",
    "*Except look:* do you notice anything strange about these counts? Inspect them closely. The word \"The\" appears \n",
    "about 20 words up from the end of the output -- and yet it also appears as the *first* entry in this output. What's \n",
    "going on here?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c0e54",
   "metadata": {},
   "source": [
    "### Investigating duplicates\n",
    "\n",
    "Let's investigate. To see whether something might be off in the way we've generated our counts, we'll look back at \n",
    "our third example, \"letter\".\n",
    "\n",
    "Let's grab the value for \"letter\" one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e844081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index == \"letter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c7839",
   "metadata": {},
   "source": [
    "That corresponds to what we have above. But remember: the start of _Frankenstein_ doesn't start with \"letter.\" It \n",
    "starts with \"Letter.\" Might this make a difference, as with \"the\"/\"The\"? Let's look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5187561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index == \"Letter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca96d5",
   "metadata": {},
   "source": [
    "Not good... something seems to be off. There appear to be multiple copies of the same word in our dataframe.\n",
    "\n",
    "To diagnose this problem, let's dig in even further. We'll search through all unique words in _Frankenstein_ and \n",
    "see whether we're somehow missing any other copies of \"letter.\" We can do so by searching through the index of our \n",
    "dataframe and testing whether \"letter\" is a substring of a given index position.\n",
    "\n",
    "```{Admonition} Reminder\n",
    "We haven't yet removed numbers from our data, so be sure to convert your indices to strings to avoid mismaches in \n",
    "datatypes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index.str.contains(\"letter\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cef7c",
   "metadata": {},
   "source": [
    "For good measure, let's do this with \"monster\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2faeaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index.str.contains(\"monster\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038cf8e",
   "metadata": {},
   "source": [
    "Wait, What's a Word?\n",
    "-------------------------\n",
    "\n",
    "The outputs above should make clear what is happening. *We have a problem in the way we've defined the concept of a \n",
    "word.* Remember, to our computers, text is just a sequence of characters. Computers are highly literal in this \n",
    "respect; they only ever read character-by-character. And while they don't have an in-built concept of what words \n",
    "are, we were able to coax them into treating character sequences as words by splitting those sequences on the basis \n",
    "of spaces. That is, we said to our computers: \"whenever you find a space, this marks the beginning or end of a \n",
    "word.\"\n",
    "\n",
    "In doing so, we ended up creating a de facto definition of what constitutes a word: for this definition, a word is \n",
    "any sequence of characters surrounded by spaces.\n",
    "\n",
    "If we frame what we've done in this way, we can see that our computers followed our definition perfectly, doing \n",
    "nothing more or less than splitting sequences of characters on spaces. In their character-by-character way of \n",
    "reading, \"letter\" is different from \"letter;\" -- and understandably so, for each is a different sequence of \n",
    "characters surrounded by spaces. The same goes for \"letter\" and \"Letter\": both are different character sequences \n",
    "surrounded by spaces, for in a very rudimentary sense, lowercase _l_ and uppercase _L_ are simply not the same \n",
    "character. (To be exact, the underlying Unicode \"[codepoints]\" for these letters are `U+006C` and `U+004C`, \n",
    "respectively.)\n",
    "\n",
    "[codepoints]: https://en.wikipedia.org/wiki/Universal_Character_Set_characters\n",
    "\n",
    "```{margin} To complicate things further...\n",
    "Non-English languages and non-alphabetic writing systems add a productive challenge to all this. We can't cover \n",
    "this topic in full, but Quinn Drombowski has written about it in this [helpful blogpost] on text analytics and the \n",
    "\"English default.\"\n",
    "\n",
    "[helpful blogpost]: http://quinndombrowski.com/?q=blog/2020/10/15/whats-word-multilingual-dh-and-english-default\n",
    "```\n",
    "\n",
    "In another sense, however, they _are_ the same letter. We could say the same of \"the\" and \"The.\" The problem here \n",
    "arises from the fact that, as opposed to our computers' highly literal way of reading, we tend to consider the \n",
    "meaning of words to be something that transcends differences in capitalization; that is mostly separable from \n",
    "punctuation; and that sometimes even goes beyond spelling (think American v. British English) and inflection \n",
    "(\"run,\" \"running,\" \"ran\" => \"run\"). In the above output, what we'd really like to see is something closer to what \n",
    "linguists call **lexemes**, or the abstract units of meaning that underlie groups of words. Otherwise, we're still \n",
    "just counting characters.\n",
    "\n",
    "The next chapter -- and with it, our first workshop session -- will discuss how to prepare textual data so as to \n",
    "begin analyzing words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
