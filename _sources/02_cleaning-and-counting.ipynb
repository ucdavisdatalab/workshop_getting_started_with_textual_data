{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3633dd89",
   "metadata": {},
   "source": [
    "Cleaning and Counting\n",
    "==================\n",
    "\n",
    "At the end of the last chapter, we caught a glimpse of the complexities involved in working with textual data. Text \n",
    "is incredibly unruly. It presents a number of challenges -- which stem as much from general truths about linguistic \n",
    "phenomena as they do from the idiosyncracies of data representation -- that we'll need to address so that we may \n",
    "formalize text in a computationally-tractable manner.\n",
    "\n",
    "As we've also seen, once we've formalized textual data, a key way we can start to gain some insight about that data \n",
    "is by counting words. Nearly all methods in text analytics begin by counting the number of times a word occurs and \n",
    "taking note of the context in which that word occurs. With these two pieces of information, **counts** and \n",
    "**context**, we can identify relationships among words and, on this basis, formulate interpretations about the texts we're studying.\n",
    "\n",
    "This chapter, then, will discuss how to wrangle the messiness of text in a way that will let us start counting. \n",
    "We'll continue with our single text file (Mary Shelley's _Frankenstein_) and learn how to prepare text so as to \n",
    "generate valuable metrics about the words within it. Later workshops will build on what we've learned here by \n",
    "applying those metrics to multiple texts.\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "+ Clean textual data with a variety of processes\n",
    "+ Recognize how these processes change the findings of text analysis\n",
    "+ Explain why you might choose to do some cleaning steps but not others\n",
    "+ Implement preliminary counting operations on cleaned data\n",
    "+ Use a statistical measure (pointwise mutual information) to measure the uniqueness of phrases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e1d67",
   "metadata": {},
   "source": [
    "Text Cleaning: Basics\n",
    "-------------------------\n",
    "\n",
    "To begin: think back to the end of the last chapter. There, we discussed a few differences between how computers \n",
    "represent and process textual data and our own way of reading. One of the key differences between these two poles \n",
    "involves details like spelling or capitalization. For us, the _meaning_ of text tends to cut across these details. \n",
    "But they make all the difference in how computers track information. Accordingly, if we want to work at a higher \n",
    "order of meaning, not just character sequences, we'll need to eliminate as many variances as possible in textual \n",
    "data.\n",
    "\n",
    "First and foremost, we'll need to **clean** our text, removing things like punctuation and handling variances in \n",
    "word casing, even spelling. This entire process will happen in steps. Typically, they include:\n",
    "\n",
    "1. Resolving word cases\n",
    "2. Removing punctuation\n",
    "3. Removing numbers\n",
    "4. Removing extra whitespaces\n",
    "5. Removing \"stop words\"\n",
    "\n",
    "Note however that _there is no pre-set way to clean text_. The steps you need to perform all depend on your data \n",
    "and the questions you have about it. We'll walk through each of these steps below and, along the way, compare how \n",
    "they alter the original text to show why you might (or might not) implement them.\n",
    "\n",
    "To make these comparisons, let's first load in _Frankenstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7414ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shelley_frankenstein.txt\", 'r') as f:\n",
    "    frankenstein = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bde13",
   "metadata": {},
   "source": [
    "We'll also define a simple function to count words. This will help us quickly check the results of a cleaning step.\n",
    "\n",
    "```{tip}\n",
    "Using `set()` in conjunction with a dictionary will allow us to pre-define the vocabulary space for which we need \n",
    "to generate counts. This removes the need to perform the `if...else` check from earlier.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0416e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    doc = doc.split()\n",
    "    word_counts = dict.fromkeys(set(doc), 0)\n",
    "    for word in doc:\n",
    "        word_counts[word] += 1\n",
    "        \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280af1a",
   "metadata": {},
   "source": [
    "Using this function, let's store the original number of unique words we generated from _Frankenstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_counts = count_words(frankenstein)\n",
    "n_unique_original = len(original_counts)\n",
    "print(\"Original number of unique words:\", n_unique_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235de26a",
   "metadata": {},
   "source": [
    "### Case normalization\n",
    "\n",
    "The first step in cleaning is straightforward. Since our computer treats capitalized and lowercase letters as two \n",
    "different things, we'll need to collapse them together. This will eliminate problems like \"the\"/\"The\" and \n",
    "\"letter\"/\"Letter.\" It's standard to change all letters to their lowercase forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1345c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = frankenstein.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab30709",
   "metadata": {},
   "source": [
    "This should reduce the number of unique words in the novel. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a452833",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_counts = count_words(cleaned)\n",
    "n_unique_words = len(cleaned_counts)\n",
    "\n",
    "print(\n",
    "    \"Unique words:\", n_unique_words, \"\\n\"\n",
    "    \"Difference in word counts between our original count and the lowercase count:\", \n",
    "    n_unique_original - n_unique_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8caf9",
   "metadata": {},
   "source": [
    "Sanity check: are we going to face the same problems from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ae05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Is 'Letter' in `normalized`?\", (\"Letter\" in cleaned), \"\\n\"\n",
    "    \"Number of times 'the' appears:\", cleaned_counts['the']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4a9e2",
   "metadata": {},
   "source": [
    "So far so good. In the above output, we can also see that \"the\" has become even more prominent in the counts: we \n",
    "found ~250 more instances of this word after changing its case (it was 3897 earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa67be",
   "metadata": {},
   "source": [
    "### Removing punctuation\n",
    "\n",
    "It's now time to tackle punctuation. This step is a bit trickier, and typically it involves a lot of going back and \n",
    "forth between inspecting the original text and the output. This is because punctuation marks have different uses, \n",
    "so they can't all be handled in the same way.\n",
    "\n",
    "Consider the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I'm a self-taught programmer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c3d4",
   "metadata": {},
   "source": [
    "```{margin} Want to practice regex?\n",
    "[regex101] offers an interactive regex viewer with lots of explanations.\n",
    "\n",
    "[regex101]: https://regex101.com/\n",
    "```\n",
    "\n",
    "It seems most sensible to remove punctuation with some combination of [regular expressions], or \"regex,\" and the \n",
    "`re.sub()` function (which substitutes a regex sequence for something else). For example, we could use regex to \n",
    "identify anything that is _not_ (`^`) a word (`\\w`) or a space (`\\s`) and remove it.\n",
    "\n",
    "That would look like this:\n",
    "\n",
    "[regular expressions]: https://en.wikipedia.org/wiki/Regular_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09eefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(re.sub(r\"[^\\w\\s]\", \"\", s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74675f",
   "metadata": {},
   "source": [
    "This method has some advantages. For example, it sticks the _m_ in \"I'm\" back to the _I_. While this isn't perfect, \n",
    "as long as we remember that, whenever we see \"Im,\" we mean \"I'm,\" it's doable -- and it's better than the \n",
    "alternative: had we replaced punctuation with a space, we would have \"I m.\" When split apart, those two letters \n",
    "would be much harder to piece back together. \n",
    "\n",
    "That said, this method also sticks \"self\" and \"taught\" together, which we don't want. It would be better to \n",
    "separate those two words than to create a new one altogether. Ultimately, this is a **tokenization** question: what \n",
    "do we define as acceptable tokens in our data, and how are we going to create those tokens? If you're interested in \n",
    "studying phrases that are hyphenated, you might not want to do any of this and simply leave the hyphens as they \n",
    "are.\n",
    "\n",
    "In our case, we'll be taking them out. The best way to handle different punctuation conventions is to process \n",
    "punctuation marks in stages. First, remove hyphens, then remove other punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d87d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.sub(r\"-\", \" \", s)\n",
    "s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f99df2",
   "metadata": {},
   "source": [
    "Let's use this same logic on `cleaned`. Note here that we're actually going to use two different kinds of \n",
    "hyphens, the en dash (-) and the em dash (—). They look very similar in plain text, but they have diferent \n",
    "character codes, and typesetters often use the latter when printing things like dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"[-]|[—]\", \" \", cleaned)\n",
    "cleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9321392",
   "metadata": {},
   "source": [
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77eb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned[:353])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e711808",
   "metadata": {},
   "source": [
    "That's coming along nicely, but why didn't those underscores get removed? Well, regex standards class underscores \n",
    "(or \"lowlines\") as word characters, meaning they class these characters along with the alphabet and numbers, \n",
    "rather than punctuation. So when we used `^\\w` to find anything that isn't a word, this saved underscores from the \n",
    "chopping block.\n",
    "\n",
    "To complete our punctuation removal, then, we'll remove these characters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfafbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"_\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb167f3",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you didn't want to do this separately, you could always include underscores in your code for handling hyphens. \n",
    "That said, punctuation removal is almost always a multi-step process, the honing of which involves multiple \n",
    "iterations. If you're interested to learn more, Laura Turner O'Hara has a [tutorial] on using regex to clean dirty \n",
    "OCR, which offers a particularly good example of how extended the process of punctuation removal can become.\n",
    "\n",
    "[tutorial]: https://programminghistorian.org/en/lessons/cleaning-ocrd-text-with-regular-expressions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bedefd",
   "metadata": {},
   "source": [
    "### Removing numbers\n",
    "\n",
    "With our punctuation removed, we can turn our attention to numbers. They should present less of a problem. While \n",
    "our regex method above ended up keeping them around, we can remove them by simply finding characters 0-9 and \n",
    "replacing them with a blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd04fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"[0-9]\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c5363",
   "metadata": {},
   "source": [
    "Now that we've removed punctuation and numbers, we'll see a significant decrease in our unique word counts. This is \n",
    "because of the way our computers were handling word differences: remember that \"letter;\" and \"letter\" were counted separately before. Likewise, our computers were counting spans of digits as words. But with all that removed, we're \n",
    "left with only words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cd3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_counts = count_words(cleaned)\n",
    "n_unique_words = len(cleaned_counts)\n",
    "\n",
    "print(\"Number of unique words after removing punctuation and numbers:\", n_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ae1af",
   "metadata": {},
   "source": [
    "That's nearly a 40% reduction in the number of unique words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fff32",
   "metadata": {},
   "source": [
    "### Text formatting\n",
    "\n",
    "Our punctuation and number removal process introduced a lot of extra spaces into the text. Look at the first few \n",
    "lines, as an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned[:76]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15788d5c",
   "metadata": {},
   "source": [
    "We'll need to remove those, along with things like newlines (`\\n`) and tabs (`\\t`). There are regex patterns for \n",
    "doing so, but Python's `split()` very usefully captures any whitespace characters, not just single spaces between \n",
    "words (in fact, the function we defined above, `count_words()`, has been doing this all along). So tokenizing our \n",
    "text as before will also take care of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f82597",
   "metadata": {},
   "source": [
    "And with that, we are back to the list representation of _Frankenstein_ that we worked with in the last chapter -- \n",
    "but this time, our metrics are much more robust. Let's use `Pandas` to make a quick dataframe and then count \n",
    "(`size()`) and sort (`sort_values()`) the words as a series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29485fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_counts = pd.DataFrame(cleaned, columns = ['WORD'])\n",
    "cleaned_counts = cleaned_counts.groupby('WORD').size().sort_values(ascending=False)\n",
    "pd.DataFrame(cleaned_counts.head(50), columns = ['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bdd25",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "With the first few steps of our text cleaning done, we can take a closer look at the output. Inspecting the 50-most \n",
    "frequent words in _Frankenstein_ shows a pattern: nearly all of them are what we call **deictics**, or words that \n",
    "are highly dependent on the contexts in which they appear. We use these constantly to refer to specific times, \n",
    "places, and persons -- indeed, they're the very sinew of language, and their high frequency counts reflect this.\n",
    "\n",
    "#### Words with high occurence\n",
    "\n",
    "We can see the full extent to which we rely on these kinds of words if we plot our counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_counts.plot(figsize = (15, 10), ylabel = \"Count\", xlabel = \"Word\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b8041",
   "metadata": {},
   "source": [
    "See that giant drop? Let's look at the 200-most frequent words and sample more words from the series index (which \n",
    "is the x axis). We'll put this code into a function, as we'll be looking at a number of graphs in this section.\n",
    "\n",
    "```{margin} How to sample xticks:\n",
    "Define a range of values from `0` to `n` (in this case, `n` will be `n_words`). Set the step count to your desired \n",
    "granularity (here we use `5`).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(word_counts, n_words=200, label_sample=5):\n",
    "    xticks_sample = range(0, n_words, label_sample)\n",
    "    word_counts[:n_words].plot(figsize = (15, 10), \n",
    "                               ylabel = \"Count\", \n",
    "                               xlabel = \"Word\",\n",
    "                               xticks = xticks_sample,\n",
    "                               rot = 90\n",
    "                              );\n",
    "\n",
    "plot_counts(cleaned_counts, n_words=200, label_sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c334f8a",
   "metadata": {},
   "source": [
    "```{margin} Further context:\n",
    "The phenomenon we are discussing here is describable in terms of [Zipf's law], which states that, for certain types \n",
    "of data, the rank-frequency distribution is an inverse relation. That is, the top-most frequent element in the data \n",
    "will occur twice as often as the second-most frequent element, which will in turn occur twice as often as the \n",
    "third-most frequent element, etc.\n",
    "\n",
    "[Zipf's law]: https://en.wikipedia.org/wiki/Zipf's_law\n",
    "```\n",
    "\n",
    "Only a few non-deictic words appear in the first half of this graph -- \"eyes,\" \"night,\" \"death,\" for example. All \n",
    "else are words like \"my,\" \"from,\" \"their,\" etc. And all of these words have screamingly high frequency counts. In \n",
    "fact, the 50-most frequent words in _Frankenstein_ comprise nearly 50% of the total number of words in the novel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fifty_sum = cleaned_counts[:50].sum()\n",
    "total_word_sum = cleaned_counts.sum()\n",
    "\n",
    "print(\n",
    "    f\"Total percentage of 50-most frequent words: {top_fifty_sum / total_word_sum:.02f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e38de1",
   "metadata": {},
   "source": [
    "The problem here is that, even though these highly-occurrent words help us say what we mean, they paradoxically \n",
    "don't seem to have much meaning in and of themselves. What can we determine about the word \"it\" without some kind \n",
    "of point of reference? How meaningful is \"the\"? These words are so common and so context-dependent that it's \n",
    "difficult to find much to say about them in and of themselves. Worse still, every novel we put through the above \n",
    "analyses is going to have a very similar distribution in terms -- they're just a general fact of language.\n",
    "\n",
    "If we wanted, then, to surface what _Frankenstein_ is about, we'll need to handle these words. The most common way \n",
    "to do this is to simply remove them, or **stop** them out. But how do we know which **stop words** to remove?\n",
    "\n",
    "#### Defining a stop list\n",
    "\n",
    "The answer to this comes in two parts. First, compiling various **stop lists** has been an ongoing research area in \n",
    "natural language processing (NLP) since the emergence of information retrieval in the 1950s. There are a few \n",
    "popular ones, like the Buckley-Salton list or the Brown list, which capture many of the words we'd think to remove: \n",
    "\"the,\" \"do,\" \"as,\" etc. Popular NLP packages like `nltk` and `gensim` even come preloaded with generalized lists, \n",
    "which you can quickly load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "gensim_stopwords = list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b688a",
   "metadata": {},
   "source": [
    "There are, however, substantial differences between stop lists, and you should carefully consider what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77aa754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of entries in `ntlk` stop list:\", len(nltk_stopwords),\n",
    "    \"\\nNumber of entries in `gensim` stop list:\", len(gensim_stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c5bfd",
   "metadata": {},
   "source": [
    "Consider, for example, some of the stranger entries in the `gensim` stop list. While it contains the usual \n",
    "suspects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['the', 'do', 'and']:\n",
    "    print(f\"{word:<3} in `gensim` stop list: {word in gensim_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7da1c1",
   "metadata": {},
   "source": [
    "...it also contains words like \"computer,\" \"empty,\" and \"thick\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd44aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['computer', 'empty', 'thick']:\n",
    "    print(f\"{word:<8} in `gensim` stop list: {word in gensim_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145ef35",
   "metadata": {},
   "source": [
    "\"Computer\" isn't likely to turn up in _Frankenstein_, but \"thick\" comes up several times in the novel:\n",
    "\n",
    "```{margin} What we're doing here:\n",
    "First, use list comprehension to find the index positions of every instance of \"thick.\"\n",
    "\n",
    "Then, for each of those index positions:\n",
    "+ Get the two words before the index (`start_span`) and the two words after it (`end_span`)\n",
    "+ Index `cleaned` with those start and end points\n",
    "+ Join the indexed selection into a string and print\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [idx for idx, word in enumerate(cleaned) if word == 'thick']\n",
    "\n",
    "for idx in idx_list:\n",
    "    start_span, end_span = idx - 2, idx + 3\n",
    "    span = cleaned[start_span : end_span]\n",
    "    print(f\"{idx:>5} {' '.join(span)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea10c51",
   "metadata": {},
   "source": [
    "This output brings us to the second, and more important part of the answer from above: **removing stop words \n",
    "depends on your texts and your research question(s)**. We're looking at a novel -- and a gothic novel at that. The \n",
    "kinds of questions we could ask about this novel might have to do with _tone_ or _style_, _word choice_, even \n",
    "_description_. In that sense, we definitely want to hold on to words like \"thick\" and \"empty.\" But in other texts, \n",
    "or with other research questions, that might not be the case. A good stop list, then, is application-specific; you \n",
    "may in fact find yourself adding _additional_ words to stop lists, depending on what you're analyzing.\n",
    "\n",
    "That all said, there are a broad set of NLP tasks that can really depend on keeping stop words in your text. These are tasks that fall under what's called **part-of-speech** tagging: they rely on stop words to parse the \n",
    "grammatical structure of text. Below, we will discuss one such example of these tasks, though for now, we'll go \n",
    "ahead with a stop list to demonstrate the result.\n",
    "\n",
    "For our purposes, the more conservative `nltk` list will suffice. Note that it's also customary to remove any \n",
    "two-character words when we're applying stop words (this prevents us from seeing things like \"st,\" or street). We \n",
    "will save the result of stopping out this list's words in a new variable, `stopped_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf72c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = cleaned_counts.index.isin(nltk_stopwords)\n",
    "stopped_counts = cleaned_counts[~to_remove]\n",
    "stopped_counts = stopped_counts[stopped_counts.index.str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d513822",
   "metadata": {},
   "source": [
    "````{tip}\n",
    "Because we were already working in `Pandas`, we're using a series subset to do this, but you could just as easily \n",
    "remove stop words with list comprehension. For example:\n",
    "\n",
    "```{code}\n",
    "[word for word in cleaned if (word not in nltk_stopwords) and (len(word) > 2)]\n",
    "```\n",
    "````\n",
    "\n",
    "With our stop words removed, let's look at our total counts and then make another count plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique words after applying `nltk` stop words:\", len(stopped_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31328756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counts(stopped_counts, n_words=200, label_sample=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab5ae2",
   "metadata": {},
   "source": [
    "#### Iteratively building stop lists\n",
    "\n",
    "This is better, though there are still some words like \"one\" and \"yet\" that it would be best to remove. The list \n",
    "provided by `nltk` is good, but it's a little _too_ conservative, so we'll want to modify it. This is perfectly \n",
    "normal: like removing punctuation, getting a stop list just right is an iterative process that takes multiple \n",
    "tries.\n",
    "\n",
    "To the `nltk` list, we'll add a set of stop words compiled by the developers of [Voyant], a text analysis portal. \n",
    "We can combine the two with a set union...\n",
    "\n",
    "[Voyant]: https://www.voyant-tools.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/voyant_stoplist.txt\", 'r') as f:\n",
    "    voyant_stopwords = f.read().split()\n",
    "    \n",
    "custom_stopwords = set(nltk_stopwords).union(set(voyant_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80ec1f",
   "metadata": {},
   "source": [
    "...refilter our counts series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be894eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = cleaned_counts.index.isin(custom_stopwords)\n",
    "stopped_counts = cleaned_counts[~to_remove]\n",
    "stopped_counts = stopped_counts[stopped_counts.index.str.len() > 2]\n",
    "\n",
    "print(\"Number of unique words after applying custom stop words:\", len(stopped_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f856f",
   "metadata": {},
   "source": [
    "...and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c3afe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_counts(stopped_counts, n_words=200, label_sample=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039576b",
   "metadata": {},
   "source": [
    "Notice how, in each iteration through these stop lists, more and more \"meaningful\" words appear in our plot. From \n",
    "our current vantage, there seems to be much more we could learn about the specifics of _Frankenstein_ as a novel by \n",
    "examining words like \"feelings,\" \"nature,\" and \"countenance,\" than if we stuck with \"the,\" \"of,\" and \"to.\"\n",
    "\n",
    "We'll quickly glance at the top 10 words in our unstopped text and our stopped text to see such differences more \n",
    "clearly. Here's unstopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddeb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cleaned_counts[:10], columns = ['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715358a",
   "metadata": {},
   "source": [
    "And here's stopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eb28f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(stopped_counts[:10], columns = ['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d49f2",
   "metadata": {},
   "source": [
    "Text Cleaning: Advanced\n",
    "-----------------------------\n",
    "\n",
    "With our stop words removed, we could consider our text cleaning to be complete. But there are two more steps that \n",
    "we could do to further process our data: stemming and lemmatizing. We'll consider these separately from the steps \n",
    "above because they entail making significant changes to our data. Instead of simply removing pieces of irrelevant \n",
    "information, as with stop word removal, stemming and lemmatizing transform the forms of words.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "**Stemming** algorithms are rule-based procedures that reduce words to their root forms. They cut down on the \n",
    "amount of morphological variance in your corpus, merging plurals into singulars, changing gerunds into static \n",
    "verbs, etc. This can be useful for a number of reasons. It cuts down on corpus size, which might be necessary when \n",
    "dealing with a large number of texts, or when building a fast search engine. Stemming also enacts a shift to a \n",
    "higher, more generalized form of words' meanings: instead of counting \"have\" and \"having\" as two different words \n",
    "with two different meanings, stemming would enable us to count them as a single entity, \"have.\"\n",
    "\n",
    "We can see this if we load the [Porter stemmer] from `nltk`. It's a class object, which we initialize by saving to \n",
    "a variable.\n",
    "\n",
    "[Porter stemmer]: https://tartarus.org/martin/PorterStemmer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9405cf",
   "metadata": {},
   "source": [
    "Let's look at a few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_stem = ['books', 'having', 'running', 'complicated', 'complicity', 'malleability']\n",
    "\n",
    "for word in to_stem:\n",
    "    print(f\"{word:<12} => {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca30fa",
   "metadata": {},
   "source": [
    "There's a lot of potential value in enacting these transformations with a stemmer. So far we haven't developed a \n",
    "method of handling plurals, which, it could be reasonably argued, should be considered the same as their singular \n",
    "variants; the stemmer handles this. Likewise, \"having\" to \"have\" is a useful transformation, and it would be \n",
    "difficult to come up with a custom algorithm that could handle the complexities of not only removing a gerund but \n",
    "replacing it with an _e_.\n",
    "\n",
    "That said, the problem with stemming is that the process is rule-based and struggles with certain words. It can \n",
    "inadvertently merge what should be two separate words, as with \"complicated\" and \"complicity\" becoming \"complic.\" \n",
    "And more, \"complic,\" like \"malleabl,\" isn't really a word. Rather, it represents a general idea, but one that a) \n",
    "is too baggy (it merges together two different words); and b) is harder to interpret in later analysis (how would \n",
    "we know what \"complic\" means when looking at word count distributions?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50adef2",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "**Lemmatizing** textual data solves some of these problems, though at the cost of more complexity and more \n",
    "computational resources. Like stemming, lemmatization removes the inflectional forms of words. While it tends to be \n",
    "more conservative in its approach, it is better at avoiding lexical merges like \"complicated\" and \"complicity\" \n",
    "becoming \"complic.\" More, the result of lemmatization is always a fully readable word, so no need to worry about \n",
    "trying to remember what \"malleabl\" means. If, say, you want to know something about the _theme_ or _topicality_ of \n",
    "a text, lemmatization would be a valuable step.\n",
    "\n",
    "#### Part-of-speech tags and dependency parsing\n",
    "\n",
    "Lemmatizers can do all this because they use the context provided by **part-of-speech tags** (POS tags). To get the \n",
    "best results, you need to pipe in a tag for each word, which the lemmatizer will use to make its decisions. In \n",
    "principle, this is easy enough to do. There are software libraries, including `nltk`, that will automatically \n",
    "assign POS tags through a process called **dependency parsing**. This proces analyzes the grammatical structure of \n",
    "a text string and tags words accordingly.\n",
    "\n",
    "But now for the catch: to work at their best, _dependency parses require both stop words and some punctuation \n",
    "marks_. Because of this, if you know you want to lemmatize your text, you're going to have to tag your text before \n",
    "doing other steps in the text cleaning process. Further, it's better to let an automatic tokenizer handle which \n",
    "pieces of punctuation to leave in and which ones to leave out. You'll still need to remove everything later on, but \n",
    "only after the dependency parser has done its work. The revised text cleaning steps would look like this:\n",
    "\n",
    "1. Tokenize\n",
    "2. Assign POS tags\n",
    "3. Resolve word casing\n",
    "4. Remove punctuation\n",
    "5. Remove numbers\n",
    "6. Remove extra whitespaces\n",
    "7. Remove stopwords\n",
    "8. Lemmatize\n",
    "\n",
    "#### Sample lemmatization workflow\n",
    "\n",
    "We won't do all of this for _Frankenstein_, but in the next session, when we start to use classification models to \n",
    "understand the differences between texts, we will. For now, we'll demonstrate an example of POS tagging using the \n",
    "`nltk` tokenizer in concert with its lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd14698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "sample_string = \"\"\"\n",
    "The strong coffee, which I had after lunch, was $3. It kept me going the rest of the day.\n",
    "\"\"\"\n",
    "tokenized = word_tokenize(sample_string)\n",
    "\n",
    "print(f\"String after `nltk` tokenization:\\n\")\n",
    "for entry in tokenized:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84fa72",
   "metadata": {},
   "source": [
    "Assigning POS tags:\n",
    "\n",
    "```{margin} What does each tag mean?\n",
    "`nltk` uses the Penn TreeBank tags, which you can find [here]. If the tagger receives a punctuation mark that isn't \n",
    "one of its special cases (\".\" or \"$\", for example), it simply repeats that mark.\n",
    "\n",
    "[here]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "print(f\"Tagged tokens:\\n\")\n",
    "for entry in tagged:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cecf3",
   "metadata": {},
   "source": [
    "Note that `nltk.pos_tag()` returns a list of tuples. We'll put these in a dataframe and clean that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f512464",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = pd.DataFrame(tagged, columns = ['WORD', 'TAG'])\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d36c8a",
   "metadata": {},
   "source": [
    "```{margin} Steps:\n",
    "1. Reassign lowercase versions of all words\n",
    "2. Subset `tagged` for all entries that aren't \",\", \"$\", or \".\"\n",
    "3. Subset `tagged` for all non-numeric numbers\n",
    "4. Subset `tagged` for words not in stop words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc790a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = tagged.assign(WORD = tagged['WORD'].str.lower())\n",
    "tagged = tagged[~tagged['WORD'].isin([\",\", \"$\", \".\"])]\n",
    "tagged = tagged[tagged['WORD'].str.isalpha()]\n",
    "tagged = tagged[~tagged['WORD'].isin(custom_stopwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8a4bd",
   "metadata": {},
   "source": [
    "Now we can load a lemmatizer -- and write a function to handle discrepancies between the tags we have up above and \n",
    "the tags that this lemmatizer expects. We did say this step is more complicated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ac5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        tag = wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        tag = wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        tag = wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        tag = wordnet.ADV\n",
    "    else:\n",
    "        tag = ''\n",
    "    return tag\n",
    "\n",
    "tagged = tagged.assign(NEW_TAG = tagged['TAG'].apply(convert_tag))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118c2c8",
   "metadata": {},
   "source": [
    "Finally, we can lemmatize.\n",
    "\n",
    "```{margin} A bit of error handling\n",
    "In case our tagger fails to assign tag, we can just send the word to our lemmatizer. The lemmatizer may have this \n",
    "word stored in its database, in which case it will make a change based on that. Otherwise, it just returns the \n",
    "word.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b388176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word, new_tag):\n",
    "    if new_tag != '':\n",
    "        lemma = lemmatizer.lemmatize(word, pos = new_tag)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma\n",
    "\n",
    "tagged = tagged.assign(\n",
    "    LEMMATIZED = tagged.apply(lambda row: lemmatize_word(row['WORD'], row['NEW_TAG']), axis=1)\n",
    ")\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4301df8",
   "metadata": {},
   "source": [
    "This is a lot of work, but it does preserve important distinctions between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complic_dict = {'complicated': 'v', 'complicity': 'n'}\n",
    "\n",
    "for word in complic_dict:\n",
    "    word, tag = word, complic_dict[word]\n",
    "    lemma = lemmatizer.lemmatize(word, pos = tag)\n",
    "    print(f\"{word:<11} => {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e2ea5",
   "metadata": {},
   "source": [
    "We are now finished with text cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d859b52",
   "metadata": {},
   "source": [
    "Chunking with N-Grams\n",
    "----------------------------\n",
    "\n",
    "The last thing we'll discuss in this session is **chunking**. Chunking is closely related to tokenization. It \n",
    "involves breaking text into multi-token spans. This is useful if, for example, we want to find phrases in our data, \n",
    "or even entities. To wit: the processes above would dissolve \"New York\" into two separate tokens, and it would be \n",
    "very difficult to know how to reattach \"new\" and \"york\" from something like raw count metrics -- we may not even \n",
    "know this entity exists in the first place. Chunking, on the other hand, would lead us to identify it.\n",
    "\n",
    "In this sense, it's often useful to count not only single words in our text, but continuous two word strings, even \n",
    "three. We call these strings **n-grams**, where _n_ is the number of tokens with which we chunk. \"Bigrams\" are \n",
    "two-token chunks. \"Trigrams\" are three-token chunks. Then, there are \"4-grams,\" \"5-grams,\" and so on. Technically, \n",
    "there's no real limit to the size of your n-grams, though their usefulness will depend on your data and your \n",
    "research questions.\n",
    "\n",
    "To finish this chapter, we'll produce bigram counts on _Frankenstein_.\n",
    "\n",
    "First, we'll return to `cleaned`, which holds the entire text in its original form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a8c19",
   "metadata": {},
   "source": [
    "This contains stop words though, so we'll take them out using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped = [word for word in cleaned if (word not in custom_stopwords) and (len(word) > 2)]\n",
    "stopped[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53901fdb",
   "metadata": {},
   "source": [
    "Once again, `nltk` has in-built functionality to help us with our chunking. There are a few options here, but since \n",
    "we want to get bigram counts, we'll use objects from `nltk`'s `collocations` module. `BigramAssocMeasures()` will \n",
    "generate our scores, while `BigramCollocationFinder()` will create our bigrams.\n",
    "\n",
    "```{margin} Other options\n",
    "`nltk` also has a `bigrams()` object that you can call with `nltk.bigrams()`. It returns an iterator of all \n",
    "bigrams, which is quite useful. That said, it's harder to use this object to produce valuable bigram metrics, as \n",
    "we'll do below.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_words(stopped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ceee03",
   "metadata": {},
   "source": [
    "If we want to get the raw bigram counts (which `nltk` calls a \"frequency distribution\"), we use the `ngram_fd` \n",
    "method. That can be stored in a dataframe, which we'll sort by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ff55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = bigram_finder.ngram_fd\n",
    "\n",
    "bigram_freq = pd.DataFrame(freq.keys(), columns = ['WORD', 'PAIR'])\n",
    "bigram_freq = bigram_freq.set_index('WORD')\n",
    "bigram_freq = bigram_freq.assign(VALUE = freq.values()).sort_values('VALUE', ascending = False)\n",
    "\n",
    "bigram_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3394b5",
   "metadata": {},
   "source": [
    "A more useful metric, however, would be a PMI, or **pointwise mutual information**, score. PMI measures the \n",
    "association strength of a pair of outcomes. In our case, the higher the score, the more likely a given bigram pair \n",
    "will be with respect to the other bigrams in which the two words of the present one appear.\n",
    "\n",
    "We can get a PMI score for each bigram using the `score_ngrams` and `pmi` methods of our collocator and measurer, \n",
    "respectively. This will return a list of nested tuples, which will take a little work to coerce into a dataframe.\n",
    "\n",
    "```{margin} Steps:\n",
    "1. Make our dataframe; the result stores a tuple in the `BIGRAMS` column\n",
    "2. Assign the first position of each tuple in `BIGRAMS` to `WORD`\n",
    "3. Assign the second position of each tuple in `BIGRAMS` to `PAIR`\n",
    "4. Remove `BIGRAMS`\n",
    "5. Rearrange our columns\n",
    "6. Set `WORDS` as our index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "pmi_df = pd.DataFrame(bigram_pmi, columns = ['BIGRAMS', 'PMI'])\n",
    "pmi_df = pmi_df.assign(\n",
    "    WORD = pmi_df['BIGRAMS'].apply(lambda x: x[0]),\n",
    "    PAIR = pmi_df['BIGRAMS'].apply(lambda x: x[1])\n",
    ")\n",
    "\n",
    "pmi_df = pmi_df.drop(columns = ['BIGRAMS'])\n",
    "pmi_df = pmi_df[['WORD', 'PAIR', 'PMI']]\n",
    "pmi_df = pmi_df.set_index('WORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f3edd",
   "metadata": {},
   "source": [
    "Let's take a quick look at the distribution of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b90b56",
   "metadata": {},
   "source": [
    "Here are the 10 bottom-most scoring bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4db008",
   "metadata": {},
   "source": [
    "And here's a sampling of 25 bigrams with a PMI above 10.31 (bigrams above the 75th percentile):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_df[pmi_df['PMI'] > 10.31].sample(25).sort_values('PMI', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13a37d",
   "metadata": {},
   "source": [
    "Among the worst-scoring bigrams, we see words that are likely to be combined with many different words: \"said,\" \n",
    "\"man,\" \"shall,\" etc. On the other hand, among the best-scoring bigrams, we see coherent entities and suggestive \n",
    "pairings. The latter especially begin to sketch out the specific qualities of Shelley's prose style."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
